# M1.2-5 – Grundlagen zu Bias in KI

## Start: Worum geht es?

Dieses Lernangebot führt in die Grundlagen von Bias in KI-Systemen ein und zeigt, wie systematische Verzerrungen in Text- und Bild-KIs entstehen, wirken und erkannt werden können. Im Mittelpunkt steht die Frage, warum KI nie vollständig neutral sein kann und welche Rolle Trainingsdaten, Algorithmen, Modellarchitekturen und Nutzungsmuster bei der Entstehung von Verzerrungen spielen.

Anhand von drei aufeinander aufbauenden Videos werden zentrale Konzepte wie Datenbias, algorithmische Verzerrungen, strukturelle Musterbildung und nutzungsbedingte Verstärkung verständlich erläutert. Dabei wird deutlich, wie KI auf statistische Erwartungen zurückgreift, wie stereotype Muster reproduziert werden können und welche einfachen Tests helfen, diese Verzerrungen sichtbar zu machen.

Das Lernangebot kombiniert theoretische Grundlagen mit praxisnahen Beispielen aus Text- und Bild-KIs. Ergänzt wird es durch konkrete Anwendungstipps, Debiasing-Strategien und kleine Übungen, die Sie direkt während der Nutzung ausprobieren können. Ziel ist ein reflektierter Umgang mit KI-Systemen, der Chancen nutzt, Risiken erkennt und Entscheidungen bewusst absichert.

Nach Abschluss können Sie erklären, was unter Bias in KI verstanden wird, wie unterschiedliche Verzerrungsformen entstehen und wie Sie durch Variantenvergleiche, kritische Rückfragen und bewusstes Prompting selbst zur Reduktion von Bias beitragen können.

Dieses Lernangebot richtet sich an Lehrkräfte, pädagogisch Verantwortliche, Multiplikator:innen und alle Personen, die KI in Schule, Bildungspraxis oder Verwaltung nutzen und die Qualität sowie Fairness von KI-Ausgaben besser einschätzen möchten.

Zur Vorbereitung empfiehlt sich ein Blick in weitere Lernangebote dieser Reihe: *M1.1 Einführung: Künstliche Intelligenz in Schule und Unterricht – Funktionsweise, Chancen und Herausforderungen* und *M1.1 Grundsätzliches Verständnis der Funktionsweise von KI*. Außerdem *M3.1 KI und Ethik: Chancen und Risiken der KI-Nutzung* und *M3.1 Ethik und Verantwortung: Zum Wechselverhältnis von Technologie und Gesellschaft*.

Das Lernangebot umfasst drei Videos mit ergänzenden Praxistipps, Reflexionsaufgaben, Transferaufgaben und vertiefenden Materialien.

---

## Testen Sie Ihr Vorwissen

Hier finden Sie Fragen zum Einstieg in das Thema.

* **Was beschreibt Bias in KI-Systemen?**
    Systematische Verzerrungen.
* **Wodurch entsteht Bias häufig?**
    Durch unausgewogene Trainingsdaten.
* **Welcher Bereich gehört NICHT zu den Bias-Ursachen?**
    Hardware des Endgeräts. (Ursachen sind hingegen Trainingsdaten und Modellarchitektur).
* **Was ist Datenbias?**
    Verzerrung durch einseitige Trainingsdaten.
* **Was bedeutet algorithmischer Bias?**
    Verzerrung durch mathematische Optimierungen.
* **Wie können Sie Bias erkennen?**
    Durch Variieren von Prompts.
* **Welche Methode hilft, Muster sichtbar zu machen?**
    Gleiche Modelle bei unterschiedlichen Anbietern vergleichen.
* **Was ist ein Debiasing-Prompt?**
    Ein Prompt, der die KI zur Reflexion von Verzerrungen anregt.
* **Welche Angabe kann Bias auslösen?**
    Ein irrelevanter Ortsname.
* **Was trifft zu?**
    Bias kann reduziert, aber nicht vollständig eliminiert werden.

**Auswertung:**
* *0-3 richtige Antworten:* Klasse, dass Sie sich an den Fragen versucht haben! Jetzt können Sie tiefer in die Materie eintauchen.
* *4-7 richtige Antworten:* Sie haben schon ein solides Grundwissen. Der Kurs wird Ihnen helfen, Themen weiter zu vertiefen.
* *8-10 richtige Antworten:* Beeindruckend! Ihr Wissen ist bereits fundiert. Der Kurs kann Ihnen eine Auffrischung und neue Impulse bieten.

---

## Grundlagen

### Bevor es los geht …

Künstliche Intelligenz wird zunehmend in Bildung, Verwaltung und Alltag eingesetzt. Wer KI nutzt, trifft Entscheidungen auf Basis algorithmischer Ergebnisse. Verzerrungen in diesen Ergebnissen können zu einseitigen Darstellungen, unpassenden Empfehlungen oder ungerechten Situationen führen. Dieses Modul hilft Ihnen, KI-Ergebnisse kritisch einzuordnen und sicherer damit umzugehen.

In diesem Kurs lernen Sie:
* was Bias in KI bedeutet,
* wie Verzerrungen entstehen,
* wie Sie Bias selbst erkennen,
* wie Sie KI-Ausgaben aktiv beeinflussen und ausgleichen,
* welche einfachen Tests Sie jederzeit anwenden können.

---

### Video: Einführung ins Thema – Was bedeutet Bias in KI-Systemen?

*(Sprechertext)*

Herzlich willkommen zu diesem Video. Wir beginnen mit einer grundlegenden Frage: Was bedeutet Bias in KI-Systemen und warum ist dieses Thema wichtig für alle, die KI einsetzen oder darüber entscheiden?

Der Begriff Bias stammt ursprünglich aus der Psychologie, der Statistik und den Sozialwissenschaften. Er bezeichnet eine systematische Verzerrung in Wahrnehmung, Beurteilung oder Datenerhebung, die dazu führt, dass Ergebnisse nicht neutral oder objektiv sind.

Eine kompakte allgemeine Definition lautet: Bias ist ein vorhersehbarer, nicht-zufälliger Fehler, der entsteht, weil bestimmte Informationen, Wahrnehmungen oder Methoden bevorzugt werden und andere systematisch vernachlässigt werden. Solche Verzerrungen können entstehen durch:
* Vorurteile oder Erwartungen,
* selektive Wahrnehmung,
* kulturelle oder soziale Prägungen,
* fehlerhafte oder unausgewogene Datenerhebung.

Bias ist also kein einmaliger Messfehler, sondern ein strukturelles Muster, das Ergebnisse konsistent in eine bestimmte Richtung verschiebt.

Solche Verzerrungen begegnen uns ständig im Alltag, in Bewertungen, Entscheidungen oder der Art, wie wir Informationen wahrnehmen. Sie entstehen, weil Menschen und Maschinen auf Muster zurückgreifen, welche die Ergebnisse systematisch beeinflussen können. Genau deshalb spielt Bias eine wichtige Rolle, wenn wir KI verantwortungsvoll einsetzen.

Wichtig ist dabei: Eine KI hat keine eigenen Absichten. Sie bildet ausschließlich Muster ab, die in ihren Daten enthalten sind. Dadurch wirken Verzerrungen in KI oft unsichtbarer, aber gleichzeitig konsequenter, weil sie sich in jeder Ausgabe wiederholen können. Gleichzeitig kann KI nie völlig neutral sein, weil Neutralität immer von gesellschaftlichen, kulturellen oder fachlichen Vorstellungen abhängt. Was in einem Kontext als ausgewogen gilt, kann in einem anderen bereits verzerrt wirken.

KI hat keine eigenen Wertmaßstäbe und folgt keinem eigenständigen Neutralitätsbegriff. Sie trifft Entscheidungen allein auf Basis von Daten, Mustern und Trainingsprozessen und kann dadurch systematische Verzerrungen reproduzieren oder verstärken.

Daher lautet eine spezifische Definition: **Bias in KI beschreibt systematische Unterschiede, die entstehen, weil Modelle die gesellschaftlichen und kulturellen Muster aus ihren Trainingsdaten abbilden und reproduzieren und dadurch bestimmte Sichtweisen oder Gruppen stärker gewichten als andere, da es keine objektive Norm gibt, an der sie sich orientieren könnten.**

Damit kennen Sie die Grundlagen. Im nächsten Video geht es darum, wie Bias in KI entsteht und welche Muster dabei eine Rolle spielen.

---

### Video: Typische Formen von Bias

*(Sprechertext)*

In diesem Video schauen wir darauf, warum Bias in KI-Modellen entsteht und welche typischen Biasformen sich daraus ableiten lassen.

**Die Rolle der Trainingsdaten (Datenbias)**
Schauen wir uns zuerst die Rolle der Trainingsdaten an. Viele Verzerrungen in KI-Systemen entstehen nicht durch das Modell selbst, sondern schon durch die Daten, aus denen die KI lernt, also durch das, was das Modell überhaupt als Datenbasis nutzt und woraus es seine Muster ableitet.
Diese Daten stammen immer aus einer bestimmten Realität. Diese Realität ist nicht neutral oder ausgewogen. Und genau deshalb entstehen Verzerrungen oft bereits im Training.

Ein Grund ist, dass bestimmte Merkmale oder Gruppen in den Daten stärker vertreten sind als andere. Das führt dazu, dass Modelle dominante Sprachvarianten, Bildmerkmale oder Altersgruppen sehr zuverlässig erkennen, während seltene Fälle schlechter generalisiert werden. Das Modell lernt vor allem das, was es häufig sieht, und behandelt andere Muster als Ausnahme.

Auch historische Verzerrungen spielen eine Rolle. Datensätze enthalten kulturelle Vorannahmen, stereotype Muster oder einseitige Darstellungen, die schon vor dem Einsatz von KI existierten. Wenn solche Muster in den Trainingsdaten stark vertreten sind, schreibt das Modell sie später einfach fort. Nicht, weil es das „möchte“, sondern weil diese Muster statistisch besonders stabil erscheinen.

In einigen Bereichen werden Trainingsdaten zusätzlich von Menschen beschriftet, etwa bei der Bildklassifikation oder der Moderation von Inhalten. Dabei entstehen Verzerrungen durch unterschiedliche Interpretationen, kulturelle Unterschiede oder unbewusste Erwartungen. Diese individuellen Einschätzungen fließen in die Daten ein und beeinflussen das Verhalten des Modells.

Ein weiterer Faktor sind unvollständige oder lückenhafte Daten. Manche Aspekte der Realität fehlen völlig oder sind nur sehr schwach vertreten. Seltene Dialekte, spezielle Situationen oder bestimmte Gruppen kommen schlicht nicht vor. Was im Training fehlt, kann das Modell später nicht zuverlässig erkennen, und genau dadurch entstehen systematische Musterverzerrungen.

Zusammengefasst lässt sich sagen: Verzerrte oder unausgewogene Trainingsdaten gehören zu den wichtigsten Ursachen für Bias in KI-Systemen. Modelle übernehmen die Strukturen ihrer Daten. Diese Form der Verzerrung bezeichnet man als **Datenbias**.

**Die Rolle der Algorithmen (Algorithmischer Bias)**
Schauen wir nun auf die Rolle der Algorithmen. Es geht darum, wie ein KI-Modell lernt, also welche mathematischen Prozesse bestimmen, welche Muster es stark gewichtet und wie es Fehler reduziert.
Auch wenn die Trainingsdaten einigermaßen ausgewogen wären, können Modelle selbst Verzerrungen erzeugen. Der Grund dafür ist die Funktionsweise des Modells: Ein Modell versucht, seine Gesamtfehlerquote zu senken und nicht Fairness oder Ausgewogenheit herzustellen.

Ein zentrales Prinzip dabei ist, dass ein Modell vor allem das lernt, was in den Trainingsdaten häufig vorkommt. Häufige Muster prägen den Lernprozess stark, weil sie das Gesamtergebnis des Modells am stärksten beeinflussen. Seltene Muster tragen dagegen wenig zur Fehlerreduktion bei und werden entsprechend schwächer gelernt.
Das bedeutet, wenn ein Muster im Datensatz sehr oft vorkommt und ein anderes nur selten, dann investiert das Modell deutlich mehr Lernkapazität in das häufige Muster. Seltene Fälle sind statistisch kaum sichtbar und fließen deshalb weniger stark in die Modellbildung ein.

Wenn Informationen aus Trainingsdaten einen bestimmten Wahrscheinlichkeits-Schwellenwert unterschreiten, dann löschen KI-Modelle ggf. diese Information, um die eigene Datenmenge gering zu halten. Aus Sicht des Betreibers des KI-Modells würden diese Informationen zu selten abgerufen bzw. berücksichtigt, dass sich eine Speicherung nicht lohnt. Dadurch werden marginalisierte Informationen algorithmisch ausgeschlossen.
Verzerrungen entstehen somit selbst dann, wenn die Daten nicht absichtlich unausgewogen sind.

Zudem orientieren sich Modelle ausschließlich an statistischen Zusammenhängen. Sie erkennen nicht, ob ein bestimmtes Merkmal inhaltlich sinnvoll oder relevant ist. Wenn ein Merkmal zufällig oft mit einem Ergebnis verknüpft ist, erhält es eine hohe Bedeutung im Modell, selbst wenn es fachlich keine Rolle spielt.

Kurz gesagt: Die Funktionsweise des Modells verstärkt Mehrheitsmuster und kann selbst bei gut zusammengestellten Daten zu systematischen Verzerrungen führen. Diese Form der Verzerrung wird als **algorithmischer Bias** bezeichnet.

**Die Modellarchitektur (Struktureller Bias)**
Schauen wir nun auf die Modellarchitektur. Es geht darum, woraus ein KI-Modell besteht, also aus welchen Bausteinen, Strukturen und Vereinfachungen die Verarbeitung von Informationen überhaupt möglich wird.

Jedes KI-Modell, egal wie groß oder komplex, arbeitet mit einer bestimmten Architektur. Sie legt fest, wie viele Schichten es gibt, wie Informationen verarbeitet werden und wie stark verschiedene Merkmale zusammengefasst oder gefiltert werden. Diese Strukturen bestimmen, welche Art von Informationen das Modell überhaupt darstellen kann und welche nicht.
Da ein KI-Modell Informationen immer durch seine Architektur verarbeitet, werden komplexe Zusammenhänge automatisch verdichtet. Selbst ausgewogene Trainingsdaten könnten diese strukturellen Begrenzungen nicht vollständig ausgleichen, weil die Architektur selbst festlegt, wie detailliert das Modell unterscheiden kann.

Hinzu kommt, dass architektonische Entscheidungen, etwa, welche Merkmale berücksichtigt werden oder wie tief oder breit das Modell aufgebaut ist, indirekt beeinflussen, was das Modell „wichtig“ findet. Wenn ein Modell bestimmte Unterschiede nicht darstellen kann, weil seine Struktur dafür zu grob ist, entsteht ebenfalls Bias.

Kurz gesagt: Die Architektur eines KI-Modells legt fest, wie Informationen verarbeitet und zu Mustern verdichtet werden. Wenn diese strukturellen Begrenzungen dazu führen, dass bestimmte Informationen systematisch schlechter abgebildet werden, spricht man von **strukturellem Bias**.

**Interaktion und Nutzung (Confirmation Bias)**
Schauen wir nun darauf, wie die Nutzung eines KI-Modells selbst Bias verstärken kann. Es geht dabei darum, wie Rückmeldungen, Nutzungsverhalten oder fortlaufende Datensammlung beeinflussen, welche Muster ein Modell zukünftig stärker oder schwächer gewichtet.

Wenn ein KI-Modell im laufenden Betrieb Daten aus Interaktionen aufnimmt, zum Beispiel Fragen, Bewertungen oder Klickverhalten, dann kann es bestehende Verzerrungen weiter ausbauen. Häufige Eingaben werden verstärkt, während seltenere oder abweichende Eingaben kaum berücksichtigt werden. Das Modell orientiert sich also zunehmend an den Mustern der aktivsten oder lautesten Nutzergruppen, nicht an einer ausgewogenen Gesamtrepräsentation.

Auch Feedback-Schleifen spielen eine wichtige Rolle: Wenn Nutzer bestimmte Antworten häufiger positiv bewerten oder auswählen, kann das Modell genau diese Muster weiter hochgewichten. Dadurch entstehen sogenannte „Feedback-Loops“, in denen vorhandene Verzerrungen immer stärker zurückgespiegelt werden. Besonders problematisch ist das, wenn diese Muster auf Vorannahmen, stereotypen Erwartungen oder einseitigen Nutzungssituationen beruhen.

Hinzu kommt, dass Modelle mit veralteten Realitäten arbeiten, wenn sie über längere Zeit nicht aktualisiert werden. Da sie nur den Wissensstand zum Zeitpunkt ihres Trainings kennen, fehlen spätere Entwicklungen, neue Sprachformen und veränderte gesellschaftliche Kontexte zwangsläufig. Dadurch bleibt das Modell bei einem veralteten Bild der Realität und verstärkt dieses Bild weiter.

Kurz gesagt: Die Art und Weise, wie KI-Modelle genutzt, bewertet und mit neuen Daten versorgt werden, kann Bias nicht nur erzeugen, sondern auch verstärken und langfristig verfestigen. Diese Verstärkung bekannter Muster wird als **Confirmation Bias** bezeichnet.

Damit haben Sie die wichtigsten Ursachen und Formen von Bias in KI-Systemen kennengelernt. Im nächsten Video geht es darum, wie Sie solche Verzerrungen erkennen und aktiv entgegenwirken können.

---

### Beispiele: Wie Algorithmen Ungleichheiten verstärken können

**Diskriminierung von Ostdeutschen durch KI-Sprachmodelle:**
KI-Sprachmodelle laufen Gefahr, bestehende gesellschaftliche Diskriminierungen zu reproduzieren. Studien zeigen, dass ostdeutsche Personen in diesen Modellen oft benachteiligt werden, da die zugrunde liegenden Trainingsdaten unausgewogen oder verzerrt sind. Die Folge sind stereotype und unfaire Ergebnisse, die Vorurteile gegenüber Ostdeutschen verstärken, anstatt sie abzubauen. Um regionale Benachteiligungen zu vermeiden, ist es von großer Bedeutung, KI-Systeme mit vielfältigen und repräsentativen Daten zu trainieren. Nur so können sie zu einem Werkzeug werden, das Chancengleichheit fördert, anstatt Ungleichheiten zu zementieren.

**Bias in der algorithmischen Bewertung von Bewerbungen:**
Der Einsatz von KI in Bewerbungsverfahren birgt das Risiko, bestehende Ungleichheiten zu verfestigen. Wenn Algorithmen mit historischen Daten trainiert werden, in denen bestimmte Gruppen über- oder unterrepräsentiert sind, neigen sie dazu, Bewerber aus den dominierenden Gruppen zu bevorzugen. Ein prominentes Beispiel ist das Amazon-Recruiting-Tool von 2018, das aufgrund seines Trainings mit überwiegend männlichen Lebensläufen Bewerbungen von Frauen systematisch benachteiligte. Solche Verzerrungen können zu einer schleichenden Diskriminierung führen und die Chancengleichheit im Bewerbungsprozess untergraben. Es ist daher unerlässlich, KI-gestützte Systeme gründlich zu überprüfen und sicherzustellen, dass sie frei von Bias sind.

**Diskriminierung durch prädiktive Polizeiarbeit:**
Auch im Justiz- und Sicherheitsbereich, wo KI-Systeme zunehmend bei Entscheidungen über Straftäter oder polizeiliche Maßnahmen unterstützen, lauert die Gefahr der Diskriminierung. Verzerrungen in Daten und Algorithmen können dazu führen, dass bestimmte Gruppen, wie ethnische Minderheiten oder sozial Benachteiligte, überproportional negativ beurteilt werden. Dies untergräbt die Unvoreingenommenheit des Rechtsstaats und gefährdet die Gleichbehandlung vor dem Gesetz. Um faire Entscheidungen zu gewährleisten und Diskriminierung zu verhindern, ist eine sorgfältige Überprüfung und Kontrolle von KI-Systemen in diesem sensiblen Bereich unabdingbar.

**Geschlechtsspezifische Verzerrungen durch KI-Systeme:**
KI-Systeme, die mit historischen Daten trainiert werden, laufen Gefahr, tradierte Geschlechterrollen und Vorurteile zu übernehmen und zu verstärken. Oft werden Frauen als weniger kompetent dargestellt, während männliche Rollen als Norm gelten. Ein Beispiel dafür ist ein Kreditvergabe-Algorithmus von Apple aus dem Jahr 2019, der aufgrund vergangener Kreditdaten Männer bevorzugte. Um dieser Art von Diskriminierung entgegenzuwirken, sind diverse Entwicklerteams und gezielte Schulungen von großer Bedeutung. Zudem müssen Trainingsdaten sorgfältig kuratiert werden, um Geschlechterstereotype abzubauen und eine faire Repräsentation zu gewährleisten. Nur so können KI-Systeme zu einem Werkzeug werden, das Gleichberechtigung fördert, anstatt Sexismus zu reproduzieren.

---

### Aufgabe: Mein eigener Confirmation Bias

Diese Übung hilft Ihnen, eigene Vorannahmen zu erkennen und bewusster mit möglichen Verzerrungen in KI-Ausgaben umzugehen. Bias entsteht nicht nur im Modell, sondern auch in unserer Interpretation der Ergebnisse.

Bitte führen Sie den folgenden kleinen Test durch, um mögliche Bestätigungstendenzen (Confirmation Bias) im Umgang mit KI-Antworten zu erkennen.

**Schritt 1 – Stellen Sie der KI diese Frage:**
„Sollte der Unterricht in Schulen grundsätzlich erst um 9 Uhr beginnen? Bitte gib eine begründete Einschätzung in 5–7 Sätzen.“
(Formulieren Sie den Prompt exakt so und lassen Sie sich die Antwort anzeigen.)

**Schritt 2 – Lesen Sie die Antwort aufmerksam durch.**
Achten Sie dabei besonders darauf:
* Welche Aussagen wirken für mich sofort plausibel?
* Welche Aussagen überlese ich oder zweifle ich eher an?
* Welche Stellen passen zu meiner eigenen Meinung?

**Schritt 3 – Reflexion: Ihre Reaktion bewusst prüfen.**
Beantworten Sie für sich die folgenden Fragen:
* Welche Argumente der KI habe ich spontan akzeptiert und warum gerade diese?
* Welche Aussagen habe ich kritisch betrachtet oder abgelehnt?
* Hätte ich dieselbe KI-Antwort auch akzeptiert, wenn sie die gegenteilige Position vertreten hätte?
* Welche eigenen Überzeugungen haben meine Bewertung beeinflusst?

---

### Video: Bias erkennen und reduzieren

*(Sprechertext)*

Um Bias in KI-Ausgaben zu erkennen, brauchen Sie keine technischen Kenntnisse. Oft reichen kleine Tests, die Sie direkt während der Nutzung durchführen können. Hier sind vier einfache Strategien.

**Prompts leicht variieren**
Ein erster Ansatz besteht darin, Prompts zu variieren. Sie stellen die gleiche Aufgabe, ändern aber ein einzelnes Wort.
Ein Beispiel: Bitten Sie die KI einmal als „Finanzberater“ und einmal als „Sozialarbeiter“, eine E-Mail zum Autokauf zu formulieren. Wenn die Rollen zu unterschiedlichen Argumentationsstilen führen, obwohl die Aufgabe gleich bleibt, deutet das auf typische Rollenmuster im Modell hin.

**Aufgabe verändern**
Eine zweite Methode ist, dieselbe Aufgabe mit einer kleinen Zusatzinformation erneut zu stellen.
Ein Beispiel: Lassen Sie sich die Tagesroutine einer „Supermarktleitung“ beschreiben – und wiederholen Sie den Prompt mit dem Zusatz „in Berlin-Kreuzberg“. Wenn der Ortsname den Fokus der Antwort verändert, zeigt das, wie stark statistische Erwartungen aus Trainingsdaten einwirken.

**Rollenzuschreibungen beobachten**
Achten Sie darauf, welche Darstellungen ein Modell automatisch erzeugt.
Ein Beispiel: Wenn bei der Generierung eines Bildes einer ‚Person in der IT-Branche‘ überwiegend männliche Darstellungen erscheinen, weist das auf stereotype Muster im Modell hin. Solche einseitigen oder ständig wiederkehrenden Merkmale sind oft klare Hinweise auf Bias.

**Ergebnisse kritisch prüfen**
Prüfen Sie, ob Antworten über Varianten hinweg gleich strukturiert bleiben oder wiederholt ähnliche Perspektiven betonen. Wenn eine Antwort unabhängig vom Kontext gleich aufgebaut ist, folgt sie vermutlich einem statistischen Muster.
Ein Vergleich verschiedener KI-Modelle kann diese Muster sichtbar machen. Unterschiedliche Trainingsdaten führen zu unterschiedlichen Ergebnissen und damit werden Verzerrungen besonders deutlich.

Nachdem wir uns angeschaut haben, wie sich Bias erkennen lässt, geht es nun darum, wie Sie Verzerrungen im Alltag aktiv reduzieren können. Auch hier reichen einfache Strategien, die Sie während der Nutzung umsetzen können.

**Bewusstes Prompting nutzen**
Sie können viel Einfluss nehmen, indem Sie die KI gezielt steuern. Bitten Sie zum Beispiel um eine neutrale, ausgewogene Darstellung oder weisen Sie das Modell an, stereotype Annahmen zu vermeiden.
Hilfreich sind auch sogenannte Debiasing-Prompts, also Hinweise oder Rückfragen, die das Modell dazu bringen, die eigenen Muster zu überprüfen.
Beispiele sind:
* „Gibt es alternative Sichtweisen?“
* „Welche Verzerrungen könnten in deiner Antwort stecken?“
* „Erstelle eine neutrale Version ohne typische Rollenzuschreibungen.“

Solche Prompts machen Antworten oft deutlich ausgewogener.

**Variantenvergleich**
Bitten Sie die KI, Ihnen zwei oder drei unterschiedliche Versionen derselben Antwort zu geben. Im Vergleich wird sichtbar, welche Elemente stabil bleiben und welche sich ändern. So können Sie einseitige Darstellungen leichter erkennen.

**Kritische Rückfragen**
Viele Verzerrungen entstehen, weil die KI Annahmen trifft oder Wissenslücken füllt. Dem können Sie entgegenwirken, indem Sie gezielte Rückfragen stellen:
* „Wie sicher ist diese Aussage?“
* „Welche Informationen fehlen dir?“
* „Was würde sich ändern, wenn die Ausgangslage anders wäre?“

Solche Rückfragen lenken das Modell zu präziseren und reflektierteren Antworten.

**Kontext bewusst nachliefern**
Wenn die Antwort zu allgemein oder stereotyp wirkt, geben Sie aktiv zusätzlichen Kontext, wie zum Beispiel:
* „Die Person verfügt über viel Erfahrung.“
* „Das Budget ist hoch.“
* „Der Ort spielt keine Rolle.“

So steuern Sie das Modell weg von statistischen Erwartungen und hin zu Ihrer tatsächlichen Situation.

Bias lässt sich nicht vollständig vermeiden, aber durch bewusste Steuerung, Rückfragen und Variantenvergleich können Sie Verzerrungen sichtbar machen und deutlich reduzieren.

---

### Prompts aus dem Video (Zusammenfassung)

**Beispiel: Austausch einzelner Schlüsselwörter**
„Stelle dich als erfahrener Finanzberater [oder: Sozialarbeiter] dar und schreibe eine E-Mail an einen Kunden über die besten Optionen für den Kauf eines neuen Autos.“

**Beispiel: Aufgabe verändern mit Zusatzinformation**
„Schreibe die Tagesroutine einer Supermarktleitung [Ergänzung: in Berlin-Kreuzberg].“

**Beispiel: Rollenzuschreibungen**
„Generiere ein Bild einer Person aus der IT-Branche.”

**Einsatz von Debiasing-Prompts**
* „Gibt es alternative Sichtweisen?“
* „Welche Verzerrungen könnten in deiner Antwort stecken?“
* „Erstelle eine neutrale Version ohne typische Rollenzuschreibungen.“

**Kritische Rückfragen**
* „Wie sicher ist diese Aussage?“
* „Welche Informationen fehlen dir?“
* „Was würde sich ändern, wenn die Ausgangslage anders wäre?“

**Kontext bewusst nachliefern**
* „Die Person verfügt über viel Erfahrung.“
* „Das Budget ist hoch.“
* „Der Ort spielt keine Rolle.“

---

## Vertiefung

### Vertiefende Informationen zum Thema

**Texte / Dokumente**
* **UNESCO – „Recommendation on Ethics of AI“:** Die Grundlagen internationaler Standards und des ersten globalen Rahmenwerks zur Regulierung von KI. Behandelt die ethischen Prinzipien von Fairness, Transparenz und Verantwortlichkeit.
* **Artikel „Forschung: KI-Sprachmodelle wie ChatGPT diskriminieren Ostdeutsche“:** Spezifische Fallstudie, die zeigt, dass KI-Modelle auch regionale Vorurteile übernehmen und reproduzieren können.
* **Artikel „KI-Systeme bevorzugen eigene Texte: Studie warnt vor 'Anti-Human-Bias'“:** Thematisiert den sogenannten „Anti-Human-Bias“ oder „KI bevorzugt KI“.
* **Artikel „Gesichtserkennung: Wie KI darüber entscheiden könnte, wer eingestellt wird“:** Reißt das Thema Vorurteile in Einstellungsprozessen durch KI an. Eignet sich gut als Aufhänger für eine Aufgabe, um Probleme der KI-Gesichtserkennung selbst zu überlegen.
* **Artikel „Benchmarks gegen KI-Bias: Sprachmodelle sollen die Vielfalt der Welt erfassen“:** Fokussiert auf Lösungsansätze wie Fairness-Benchmarks und die Notwendigkeit, über Einheitsdefinitionen von Fairness hinauszugehen.
* **Text „KI und Sexismus: Klischees im Code“:** Text, der die Benachteiligung von Frauen durch KI-Modelle anhand von Beispielen (Amazon-Recruiting-Tool, Apple-Kreditkarte) beleuchtet.
* **Text „Künstliche Intelligenz hat Vorurteile gegenüber Dialekten“:** Kurzer Text zu einer Studie, die Vorurteile von KIs gegenüber Dialekten bei der Textbewertung feststellt – ein weiteres Beispiel für spezifischen Bias in Sprachmodellen.
* **Text „Künstliche Intelligenz im Migrationsmanagement“:** Text mit Infos zu Herausforderungen und Kritik im Kontext des Einsatzes von KI in sensiblen Bereichen wie dem Migrationsmanagement.
* **Text „Von Software-Beton, falschen Vorhersagen und 'intelligenter' Diskriminierung“:** Text zur Ordnung von Menschen und Lebensräumen durch digitale Entscheidungsarchitekturen und der damit verbundenen Diskriminierung.
* **Text „Diskriminierung“:** Erläutert, dass Algorithmen nicht neutral sind und strukturelle Ungleichheit reproduzieren können. Erwähnt Beispiele wie den COMPAS-Algorithmus und Deepfakes.

**Audio / Podcast**
* **Podcast „Software-Testing: Mensch vs. Maschine – wer urteilt fairer?“:** Beschreibung zu einem Podcast über Vertrauen, Vorurteile und Fairness in algorithmischen Entscheidungen.
* **Audio „Wie Künstliche Intelligenz Rollenbilder zementiert“:** Audiosource zum Thema Sexismus und Rollenbilder in KI. Zeigt, wie Algorithmen Klischees aus den Trainingsdaten übernehmen und reproduzieren.
* **Text/Audio „Wenn Künstliche Intelligenz Bürger verwaltet“:** Thematisiert den Einsatz von KI in Behörden (Arbeitsamt) und die damit verbundenen ethischen Fragen und die Gefahr der Diskriminierung.
* **Audio „Diskriminierung durch KI: Welche Vorurteile hat Künstliche Intelligenz?“:** Audiobeitrag, der mit der Überzeugung aufräumt, KI sei neutral, und zeigt, wie Vorurteile in Code und Daten verbaut sind.
* **Deutschlandfunk Kultur: American Smile – Das Lächeln der KI:** Dieser Podcast widmet sich einem spannenden Phänomen: Warum lächeln KI-generierte Gesichter fast immer auf eine bestimmte, „amerikanische“ Weise? Ein hörenswerter Beitrag über kulturelle Verzerrungen (Bias) in Trainingsdaten und deren Folgen für unsere Wahrnehmung.

---

### Stimmen aus der Praxis

**Inwiefern verstärkt KI in sozialen Medien Vorurteile (Bias), und auf welche Weise zeigt sich das?**

Um Ihnen einen authentischen Einblick in die praktische Anwendung von KI zu geben, haben wir Expert*innen aus verschiedenen Bereichen zu Wort kommen lassen.

---

## Transferaufgaben

### Selbstreflexion: Wo begegnet Ihnen Bias im Alltag?

Bitte nehmen Sie sich einige Minuten Zeit und überlegen Sie:

1.  Wo sind Sie in den letzten Tagen Situationen begegnet, in denen Entscheidungen, Bewertungen oder Informationen möglicherweise verzerrt waren? (z. B. Nachrichten, Social Media, Alltagssituationen, Einschätzungen über Personen oder Tätigkeiten)
2.  Welche dieser Verzerrungen waren menschlich und welche könnten KI-basiert gewesen sein?
3.  Welche Rolle spielt Ihr eigener Kontext (Beruf, Erfahrungen, Werte) für Ihre Wahrnehmung?

### Transferaufgabe: Bias-Check in der Praxis

In dieser Aufgabe geht es darum, Bias in KI-Modellen zu erkennen und mit einem gezielten Debias-Prompt zu reduzieren.

**Schritt 1 – Bildgenerierung**
Beginnen Sie mit einer ersten Bildgenerierung mithilfe eines bewusst unspezifischen Prompts. Nutzen Sie dazu die Formulierung:
*„Erstelle ein Bild einer Gruppe wohlhabender Menschen in einem luxuriösen Saal.“*
Dieser offene Prompt lässt viel Interpretationsspielraum und macht es möglich, dass im erzeugten Bild implizite Modellannahmen sichtbar werden.

**Schritt 2 – Analyse**
Betrachten Sie das erzeugte Bild aufmerksam und prüfen Sie, welche Personen dargestellt werden, welche Altersgruppen, Hautfarben oder Geschlechter vorkommen und welche fehlen. Achten Sie darauf, welche sozialen oder kulturellen Vorstellungen der Szene zugrunde liegen und ob bestimmte stereotype Muster besonders sichtbar sind.

**Schritt 3 – Debiasing-Prompt**
Erstellen Sie nun eine zweite Version des Bildes, die gezielt weniger einseitig sein soll. Verwenden Sie dafür folgenden Debias-Prompt:
*„Erstelle eine zweite Version des Bildes. Die Gruppe soll divers in Alter, Geschlecht und Herkunft sein und realistische Darstellungen von Wohlstand zeigen. Vermeide stereotype oder einseitige visuelle Zuordnungen.“*
Durch diese erneute Bildgenerierung wird sichtbar, wie sich die Darstellung verändert, wenn das Modell bewusst in Richtung Vielfalt und Ausgewogenheit gelenkt wird.

**Schritt 4 – Vergleich & Reflexion**
Vergleichen Sie die beiden Bildversionen. Beobachten Sie, wie sich die Darstellung der Personen verändert hat, welche zusätzlichen Perspektiven oder Gruppen sichtbar geworden sind und welche stereotypen Muster aus der ersten Version abgeschwächt oder korrigiert wurden.
Reflektieren Sie abschließend, an welchen Stellen Bias im ersten Bild besonders deutlich wurde und wie stark der Debias-Prompt zur Veränderung der Darstellung beigetragen hat.

Wenn Sie möchten, können Sie weitere Prompts ausprobieren und beobachten, wie unterschiedliche Formulierungen die Darstellung verändern und welche Muster der KI dabei immer wieder sichtbar werden. Sie können diese Erkundung auch auf Textgenerierung übertragen und prüfen, ob sich ähnliche Stereotype oder einseitige Muster in Beschreibungen, Geschichten oder Erklärtexten zeigen und wie sich diese durch gezieltes Debiasing-Prompting verändern lassen.

---

## Zusammenfassung & Glossar

### Zusammenfassung

In diesem Modul haben Sie die Grundlagen von Bias in KI-Systemen kennengelernt. Ausgangspunkt war der Begriff Bias als systematische Verzerrung, die nicht zufällig entsteht, sondern auf wiederkehrenden Mustern basiert. Sie haben gesehen, dass solche Verzerrungen sowohl in menschlichen Wahrnehmungen als auch in Daten, Methoden und technischen Systemen auftreten.

Anschließend haben Sie erfahren, wie Bias in KI entsteht: durch unausgewogene oder stereotype Trainingsdaten, durch algorithmische Optimierungsprozesse, durch die Struktur des Modells sowie durch die Art und Weise, wie KI genutzt und weitertrainiert wird. Verzerrungen können dadurch nicht nur entstehen, sondern sich über viele Ausgaben hinweg verstärken.

Im dritten Schritt ging es darum, wie Sie Bias erkennen und im Alltag sichtbar machen können. Sie haben einfache Teststrategien kennengelernt – z. B. das Variieren von Prompts, das Austauschen einzelner Informationen, die bewusste Beobachtung von Rollenbeschreibungen oder den Vergleich verschiedener KI-Modelle. Auch Maßnahmen zur Reduktion von Bias – wie bewusste Formulierungen, Debiasing-Prompts und Variantenvergleiche – wurden vorgestellt.

Insgesamt zeigt sich: Bias lässt sich nie vollständig vermeiden, aber bewusst nutzen. Mit kritischem Blick, kleinen Tests und reflektierter Anwendung können Sie Verzerrungen erkennen, einordnen und ihre Auswirkungen deutlich reduzieren. So tragen Sie dazu bei, KI verantwortungsvoll, fair und transparent einzusetzen.

### Glossar

Hier finden Sie unser Glossar zum Thema: Glossar KI

---

## KI-Transparenzhinweis

Wir praktizieren, was wir lehren! Dieses Lernangebot wurde bewusst mit KI-Technologien entwickelt – als lebendiges Beispiel für die Möglichkeiten moderner künstlicher Intelligenz.

Unsere KI-Assistenten waren unsere Co-Kreativen:
* Perplexity AI gestaltete Struktur und Inhalte
* Gemini und NotebookLM recherchierte unterstützende Quellen
* ChatGPT schrieb Textentwürfe
* DeepL verfeinerte die Sprache

So zeigen wir: KI ist mehr als eine Technologie – sie ist ein mächtiges Werkzeug für Kreativität und Wissensarbeit. Eines bleibt dabei unverrückbar: Die menschliche Expertise und Verantwortung bilden die Grundlage.

**Letzte Aktualisierung**
Dieses Lernangebot wurde im September 2025 fertiggestellt und ist danach nicht mehr aktualisiert worden. Es liegt in der Natur des Internets, dass Webangebote und Links sich verändern oder nicht mehr verfügbar sind. Insofern bitten wir um Nachsicht, falls ein genanntes und/oder verlinktes Angebot nicht mehr erreichbar ist oder sich verändert hat.

**Kontakt**
Wenn Sie eine Frage oder eine Rückmeldung zu diesem Lernangebot haben, freuen wir uns, wenn Sie uns im Forum oder Christian Haake über christian.haake@nlq.niedersachsen.de kontaktieren.

**Team**
* Verantwortlich seitens des NLQ: Christian Haake und Jörg Steinemann
* Konzeption, Redaktion: Jöran Muuß-Merholz, Blanche Fabri, Nicole Hagen, Frank Homp, Tessa Moje der Agentur J&K – Jöran und Konsorten
* Gesamtleitung: Blanche Fabri, Agentur J&K – Jöran und Konsorten
* Mitarbeit: Jula Henke, Pascal Fieseler, Ben Paetzold der Agentur J&K – Jöran und Konsorten

**Lizenzhinweise**
Dieses Angebot ist frei lizenziert und im Sinne von Open Educational Resources (OER) offen zur weiteren Verwendung, Veränderung, Weitergabe etc. Als Gesamtwerk steht er unter der Lizenz CC BY 4.0. Als Namensnennung im Sinne der Lizenz ist vorgesehen: „Agentur J&K – Jöran und Konsorten im Auftrag des Niedersächsischen Landesinstituts für schulische Qualitätsentwicklung (NLQ Hildesheim)“.

Einzelne Elemente des Angebots, zum Beispiel Abbildungen, Videos, Texte, können eigenständig unter anderen Lizenzbedingungen freigegeben sein, auch durch Dritte. In diesen Fällen ist dies im oder am jeweiligen Element angegeben.