# Computational Propaganda: Definition, Mechanismen und Akteure

## Worum geht es?

In diesem Lernangebot erkunden Sie die grundlegenden Methoden algorithmischer Desinformation: Social Bots, Deepfakes, Filterblasen und psychografisches Targeting. Sie erfahren, wie automatisierte Programme in sozialen Netzwerken Inhalte posten, liken und teilen, um Meinungen zu beeinflussen, wie KI-gestützte Deepfakes echte Beiträge imitieren und welche Rolle personalisierte Algorithmen bei der Bildung von Filterblasen spielen.

Ziel ist es, Ihre Medienkompetenz zu stärken: Sie erkennen manipulative Mechanismen, verstehen ihre Wirkungsweisen und lernen, technische und pädagogische Gegenstrategien anzuwenden. Vorkenntnisse in sicherer Internetnutzung und Grundwissen digitaler Medien werden vorausgesetzt.

## Grundlagen

Als Lehr- oder Verwaltungskraft prägen Sie wesentlich die Medienkompetenz junger Menschen. In einer Zeit, in der digitale Desinformationskampagnen zunehmend öffentliche Debatten verzerren, sind Sie gefordert, Lernende darin zu schulen, manipulative Inhalte zu erkennen und kritisch zu hinterfragen.

Zunächst erhalten Sie einen kompakten Überblick über die Mechanismen algorithmisch gesteuerter Desinformation: Sie erfahren, wie Social Bots in sozialen Netzwerken agieren, Deepfakes durch KI erstellt werden und personalisierte Filterblasen entstehen. Anschließend lernen Sie, welche psychologischen Trigger und Targeting-Strategien (Behavioral, Emotional, Psychografisch) genutzt werden, um Nutzer:innen individuell anzusprechen.

Darauf aufbauend erkunden Sie praxisorientierte Methoden zur Erkennung und Analyse: Sie wenden Strukturanalyse und Verhaltensanalyse an, um Bot-Netzwerke zu identifizieren, und nutzen forensische Techniken, um Deepfakes zu entlarven. Im nächsten Schritt reflektieren Sie Ihre eigenen Mediennutzungsmuster und entwickeln Transferaufgaben für den Schulalltag. Zum Abschluss vertiefen Sie Ihre Expertise, indem Sie Governance-Ansätze und internationale Initiativen kennenlernen und eigene Konzepte zur Stärkung der digitalen Resilienz erarbeiten.

---

### Einführung in das Thema

Willkommen zum Lernangebot „Computational Propaganda: Definition, Mechanismen und Akteure“. Im Folgenden geht es um ein Thema, das uns alle im Netz ständig betrifft. Vielleicht haben Sie schon selbst erlebt, wie online versucht wird, Ihre Meinung gezielt zu beeinflussen – manchmal subtil, manchmal sehr offensichtlich. Genau darum geht es hier. Wir stützen uns auf fundierte Quellen, darunter Berichte des Oxford Internet Institute, Analysen der Bundeszentrale für politische Bildung und aktuelle Studien zu KI und Desinformation.

Warum das wichtig ist? Weil diese Techniken demokratische Prozesse beeinflussen können – zum Beispiel Wahlen – und direkt im Social-Media-Feed eines jeden und einer jeden auftauchen, oft ohne dass dies bemerkt wird.

**Was ist Computational Propaganda?**
Im Kern bedeutet Computational Propaganda den gezielten Einsatz digitaler Technologien – also Algorithmen, automatisierte Programme (Bots) und immer mehr auch künstliche Intelligenz (KI) –, um Online-Meinungen zu beeinflussen. Das geschieht häufig mit irreführenden oder sogar falschen Informationen und vor allem dort, wo die Reichweite am größten ist: in sozialen Netzwerken. Das Entscheidende ist nicht nur die einzelne Lüge, sondern die systematische, oft automatisierte und skalierbare Beeinflussung. Es geht um koordinierte Aktionen, bei denen viele Bots oder Accounts gemeinsam Diskurse lenken oder stören – Propaganda im Industriemaßstab.

**KI hebt Computational Propaganda auf ein neues Level:**
* Die massenhafte Inhaltserstellung mit KI kann Texte, Bilder und Videos generieren, die täuschend echt wirken.
* Inhalte werden mithilfe von KI personalisiert auf dich und deine Interessen zugeschnitten, um maximale Wirkung zu erzielen.
* KI erkennt, was dich emotional anspricht, und nutzt das durch gezielte Ansprache.
* Hochentwickelte Bots imitieren echte Nutzende, verbreiten Inhalte, verstärken Hashtags und simulieren Unterstützung – das nennt man **Astroturfing**.

**Wer steckt dahinter?**
Hinter der Technologie stehen immer Menschen oder Organisationen: Regierungen, Parteien, Interessengruppen oder finanzstarke Einzelpersonen. Sie nutzen diese Werkzeuge, um Wahlen zu beeinflussen, Gegner zu diskreditieren oder Misstrauen zu säen – letztlich geht es um Macht und Deutungshoheit im Netz.

**Konkrete Beispiele aus der Praxis:**
* **US-Präsidentschaftswahl 2016:** Fast ein Fünftel aller Tweets kam von Bots. Russische Akteure streuten gezielt Desinformation, um die öffentliche Meinung zu beeinflussen.
* **Wahlen in Brasilien und Ukraine-Konflikt:** Auch hier wurden Bot-Netzwerke und Fake-Accounts eingesetzt, um politische Narrative zu verstärken.
* **Gesundheitsbereich:** Anti-Impfkampagnen und Falschinformationen zu Covid-19 wurden mit KI-gestützten Methoden verbreitet.
* **Gaza-Konflikt 2023/24:** Es tauchten KI-generierte, emotionalisierende Bilder in sozialen Netzwerken auf.

**Die gesellschaftlichen Folgen**
Kampagnen fördern Echokammern und Filterblasen: Einem werden fast nur noch Inhalte gezeigt, die die eigene Meinung bestätigen. Das führt zu mehr Polarisierung und schwindendem Vertrauen in Medien und Institutionen. In einer sogenannten "Post-Truth"-Gesellschaft zählen Fakten weniger als Emotionen oder gefühlte Wahrheiten.

**Herausforderungen im Umgang**
Herausfordernd ist, dass sich die Technologie rasant weiterentwickelt – oft schneller als die Regeln. Täter bleiben anonym, agieren grenzüberschreitend, und es fehlen klare rechtliche Rahmenbedingungen. Es ist ein Wettrüsten zwischen Angreifern und Verteidigern.

**Was kann man tun?**
Es gibt kein Patentrezept, aber einen Mix aus Strategien:
1.  **Technologisch:** KI kann genutzt werden, um Bots und Fakes zu erkennen, Faktencheck-Tools zu verbessern und Desinformation schneller aufzudecken.
2.  **Gesellschaftlich:** Die Medienkompetenz muss gestärkt, kritisches Denken gefördert und Bildung ausgebaut werden, damit wir alle lernen, Quellen zu prüfen und nicht alles sofort zu glauben.
3.  **Politisch und rechtlich:** Es braucht stärkere Forderungen nach mehr Transparenz der Plattformen, klare Regeln für politische Online-Werbung und den Einsatz von Bots sowie eine Verstärkung der internationalen Zusammenarbeit.

Selbst wenn Sie glauben, nicht direkt Ziel einer Kampagne zu sein: Schon das Wissen um diese Manipulationsmöglichkeiten verändert Ihr Online-Verhalten und Ihr Vertrauen in das, was Sie sehen und lesen. Macht Sie das kritischer, misstrauischer oder vielleicht sogar zynischer? Das ist eine wichtige Frage für jeden von uns.

---

### Exkurs: Studien und Hintergründe

**Studie des Oxford Internet Institute**
Eine wegweisende Studie unter Leitung von Samuel Woolley und Philip Howard analysierte den Einsatz von Computational Propaganda in neun Ländern. Das Forscherteam untersuchte Millionen von Posts auf sieben Social-Media-Plattformen und führte Experteninterviews durch. Sie argumentieren, dass Computational Propaganda nicht isoliert betrachtet werden kann, sondern als komplexes Phänomen in einem globalen Umfeld verstanden werden muss.

**Manipulation von Wahlen**
Eine Studie im Journal "First Monday" dokumentierte systematisch den Einfluss von Social Bots auf die US-Präsidentschaftswahl 2016. Etwa ein Fünftel aller wahlbezogenen Tweets stammten von automatisierten Accounts. In Brasilien zeigten Untersuchungen, wie Bot-Netzwerke auch nach Wahlen aktiv blieben, um politische Opposition aufrechtzuerhalten.

**COVID-19 Fehlinformationen**
Studien zeigen, dass 53–66 % der bekannten Bot-Accounts aktiv über COVID-19 tweeteten und als "Super-Spreader" von Fehlinformationen fungierten.

**Propaganda als Informationskriegs-Tool**
Im Russland-Ukraine-Konflikt wurden soziale Medien systematisch als Werkzeug der Informationskriegsführung genutzt. Beide Konfliktparteien verwendeten soziale Medien zur Meinungsbildung und Verbreitung von Desinformation.

**KI-generierte Bilder in Konflikten**
Ein Beispiel für KI-Missbrauch deckte CORRECTIV auf: Ein emotional aufgeladenes Bild eines Mannes mit fünf Kindern in Gaza wurde als authentisches Foto verbreitet, war jedoch KI-generiert. Experten identifizierten es anhand typischer Artefakte wie fehlenden Zehen und anatomischen Unstimmigkeiten.

---

## Wie funktionieren Social Bots und KI-gestützte Desinformationskampagnen?

Im Folgenden gehen wir darauf ein, wie Social Bots und KI-gestützte Desinformationskampagnen eigentlich funktionieren. Das ist quasi der Blick unter die Motorhaube.

**Definition von Social Bots**
Social Bots sind im Grunde automatisierte Computerprogramme, die sich in sozialen Netzwerken wie echte Menschen verhalten. Der Name kommt von „Social“ für soziale Medien und „Bot“ – eine Kurzform von „Robot“. Diese Programme sind darauf programmiert, menschliches Verhalten zu simulieren: Sie posten Inhalte, kommentieren, liken, teilen und interagieren mit anderen Nutzern, als wären sie echte Personen.
Das Perfide daran: Die meisten Social Bots sind so programmiert, dass Sie als Nutzer gar nicht merken, dass Sie gerade mit einer Maschine interagieren. Sie haben oft täuschend echte Profile mit Fotos, Biografien und einer Posting-Geschichte, die menschlich wirkt.

**Funktionsweise**
* **Einfache Bots:** Arbeiten mit simplen Wenn-dann-Regeln. *Wenn* ein bestimmtes Schlüsselwort oder Hashtag auftaucht, *dann* posten sie vorgefertigte Antworten oder verstärken bestimmte Inhalte durch Likes und Shares.
* **Komplexere Bots:** Nutzen künstliche Intelligenz und maschinelles Lernen. Sie können aus verschiedenen Onlinetexten neue Kommentare zusammenstellen, sich auf das aktuelle Tagesgeschehen beziehen und ihre Posting-Zeiten variieren, um menschlicher zu wirken. Sie bauen sogar geplante Pausen und Tippfehler ein, um authentisch zu erscheinen.

**Netzwerkeffekte und Astroturfing**
Einzelne Bots sind relativ harmlos – gefährlich werden sie erst, wenn sie in koordinierten Netzwerken agieren. Stellen Sie sich vor: Tausende von Bots verstärken gleichzeitig dieselbe Botschaft, liken dieselben Posts oder pushen dieselben Hashtags. Das kann den Eindruck erwecken, als wäre ein Thema plötzlich sehr populär – obwohl dahinter nur automatisierte Programme stecken.
Diese Technik nennt sich **Astroturfing**: Es soll aussehen wie eine echte Graswurzelbewegung, ist aber künstlich erzeugt. Bot-Netzwerke können so Trends manipulieren und der Öffentlichkeit vorgaukeln, dass bestimmte Meinungen weit verbreitet sind.

**KI und Desinformation**
Moderne KI-Systeme bringen Desinformationskampagnen auf ein neues Level. Sie können personalisierte Inhalte in Echtzeit erstellen, die auf Ihre Interessen zugeschnitten sind, verschiedene Sprachen und Dialekte verwenden, emotionale Trigger gezielt einsetzen und Bilder und Videos manipulieren oder komplett künstlich erstellen.
Die Qualität wird dabei immer besser. Studien zeigen, dass KI-generierte Propaganda-Texte Menschen ähnlich stark beeinflussen können wie von Menschen geschriebene Texte. Das macht es für Sie als Nutzer immer schwieriger, echte von künstlich erzeugten Inhalten zu unterscheiden.

**Erkennungsmethoden**
Forscher arbeiten an verschiedenen Methoden zur Bot-Erkennung:
* **Strukturanalyse:** Untersucht die Vernetzung und Aktivitätsmuster zwischen Accounts, um verdächtige Verbindungen aufzudecken.
* **Verhaltensanalyse:** Betrachtet Posting-Zeiten, Interaktionsmuster und Sprachmerkmale, da Bots oft unnatürliche Aktivitätsrhythmen zeigen.
* **Maschinelles Lernen:** Nutzt KI-Systeme, die durch Trainingsdaten bot-typisches Verhalten erkennen und dabei kontinuierlich ihre Erkennungsgenauigkeit verbessern.

Auch Sie können auf verdächtige Anzeichen achten: Profile ohne echte Fotos, sehr einseitige Posting-Geschichte, unnatürlich hohe Aktivität oder repetitive Formulierungen können Hinweise auf Bots sein.

**Technologisches Wettrüsten**
Während die Erkennungstechnologien besser werden, entwickeln sich auch die Bots weiter. Neue KI-Modelle können immer überzeugendere menschliche Kommunikation simulieren. Das bedeutet: Auch wenn wir heute bestimmte Bots erkennen können, werden die von morgen wahrscheinlich noch schwieriger bis gar nicht mehr zu entlarven sein.

---

## Personalisierung und Targeting: Wie werden Sie zur Zielscheibe?

Lassen Sie uns jetzt in die Welt der personalisierten Manipulation eintauchen. Hier geht es darum, wie KI-Systeme Sie als Individuum ins Visier nehmen und maßgeschneiderte Desinformation erstellen, die genau auf Sie zugeschnitten ist.

**Ihre Daten: Der Rohstoff für Manipulation**
Jeden Tag hinterlassen Sie digitale Spuren: Was Sie suchen, welche Artikel Sie lesen, wie lange Sie bei bestimmten Inhalten verweilen, was Sie liken, teilen oder kommentieren. Diese Datensammlung läuft oft völlig unbemerkt ab – über Cookies, Tracking-Pixel, Social-Media-Buttons und sogar über die technischen Eigenschaften Ihres Geräts (Browser-Fingerprinting).
All diese Informationen werden zu einem detaillierten Profil von Ihnen zusammengeführt. Algorithmen und KI-gestützte Systeme analysieren dabei nicht nur offensichtliche Daten wie Alter oder Wohnort, sondern leiten auch Ihre Persönlichkeitsmerkmale, politischen Überzeugungen und emotionalen Schwachstellen ab.

**Tiefe Einblicke in Ihre Persönlichkeit**
Moderne KI-Systeme sind erschreckend gut darin, aus scheinbar harmlosen Daten tiefere Einblicke zu gewinnen.
* Sie analysieren Ihr Klickverhalten (Was lesen Sie bis zum Ende?).
* Sie analysieren Ihre Interaktionsmuster (Mit wem vernetzen Sie sich?).
* Sie analysieren Ihre Sprache (Wie emotional schreiben Sie?).
* Sie analysieren Ihr Timing (Wie reagieren Sie unter Stress?).

Aus diesen Daten können Algorithmen ableiten, ob Sie eher ängstlich oder selbstbewusst sind, ob Sie zu Verschwörungstheorien neigen oder wie Sie politisch denken.

**Targeting-Strategien**
Basierend auf Ihren Profildaten können Manipulatoren verschiedene Strategien anwenden:
1.  **Behavioral Targeting:** Ihre Handlungen im Netz werden analysiert und vorhergesagt.
2.  **Emotionales Targeting:** KI erkennt, welche emotionalen Trigger bei Ihnen funktionieren (z. B. Angst, Wut, Hoffnung) und spricht diese gezielt an.
3.  **Psychografisches Targeting:** Hier geht es um Ihre Werte, Einstellungen und Persönlichkeitsmerkmale (z. B. traditionell, rebellisch, hilfsbereit).

Besonders raffiniert wird es, wenn die Personalisierung in Echtzeit passiert. KI-Systeme analysieren Ihr aktuelles Verhalten und passen die Inhalte sofort an.

**Filterblasen und Echokammern**
Diese Personalisierung führt zu zwei problematischen Phänomenen:
* **Filterblasen:** Entstehen automatisch durch Algorithmen. Sie zeigen Ihnen vorwiegend Inhalte, die Ihren bisherigen Interessen entsprechen. Sie verpassen andere Perspektiven wie durch einen unsichtbaren Filter.
* **Echokammern:** Gruppen von Menschen mit ähnlichen Ansichten, die sich gegenseitig bestätigen. In sozialen Medien werden Sie algorithmisch mit Gleichgesinnten zusammengeführt.
Der Unterschied ist wichtig: Filterblasen entstehen durch technische Algorithmen, Echokammern durch bewusste Gruppendynamik. Beide verstärken sich aber gegenseitig.

**Mikrotargeting**
Mikrotargeting ist die Manipulation im Miniaturformat. Hier werden Menschen nicht nur als Teil einer großen Zielgruppe angesprochen, sondern fast schon individuell. Stellen Sie sich vor: Von einer Million Menschen sehen nur Sie und ein paar hundert andere eine ganz spezielle Botschaft, die genau auf Ihre Ängste und Hoffnungen zugeschnitten ist. Das macht diese Art der Manipulation so gefährlich: Sie ist kaum nachweisbar, da die Botschaften nur einem winzigen Kreis gezeigt werden, aber extrem wirkungsvoll.

**Predictive Analytics**
KI-Systeme versuchen auch vorherzusagen, wie Sie sich verhalten werden. Sie prognostizieren, wie wahrscheinlich es ist, dass Sie eine Nachricht teilen, welche Art von Inhalt Sie emotional bewegt und wann Sie am empfänglichsten für Botschaften sind.

**Schutzmaßnahmen**
Auch wenn die Technologie mächtig ist – Sie sind ihr nicht hilflos ausgeliefert.
* Nutzen Sie bewusst verschiedene Quellen und suchen Sie unterschiedliche Perspektiven.
* Überprüfen Sie regelmäßig Ihre Datenschutzeinstellungen.
* Nutzen Sie Tracking-Blocker und Cookie-Verweigerung.
* Hinterfragen Sie kritisch, warum Ihnen bestimmte Inhalte angezeigt werden.

Das Wichtigste aber ist das Bewusstsein für diese Mechanismen. Wenn Sie wissen, wie die Manipulation funktioniert, können Sie sich besser davor schützen.

---

## Gesellschaftliche Auswirkungen und Strategien zur Stärkung der Resilienz

Was bedeutet Computational Propaganda für unsere Gesellschaft und unsere Demokratie? Und vor allem: Wie können wir uns als Gesellschaft dagegen wappnen?

**Bedrohung demokratischer Prinzipien**
Computational Propaganda ist mehr als nur ein technisches Problem. Wenn Meinungsbildung systematisch manipuliert wird, geraten zentrale Prinzipien in Gefahr:
* **Informierte Meinungsbildung:** Wenn die Informationsgrundlage durch Desinformation vergiftet wird, können Bürgerinnen und Bürger keine informierten Entscheidungen mehr treffen.
* **Vertrauen in Institutionen:** Desinformationskampagnen zielen oft darauf ab, das Vertrauen in Medien, Wissenschaft und Staat zu untergraben.
* **Gesellschaftlicher Zusammenhalt:** Polarisierung und Spaltung schwächen die Widerstandskraft einer Gesellschaft.

**Gesellschaftliche Resilienz**
Resilienz beschreibt die Fähigkeit einer Gesellschaft, Krisen zu überstehen und gestärkt aus ihnen hervorzugehen. Forscher haben verschiedene Faktoren identifiziert, die dazu beitragen:
* **Medienkompetenz:** Je besser Menschen digitale Medien verstehen, desto weniger anfällig sind sie. Das umfasst das Verstehen von Algorithmen, das Bewerten von Quellen und das Erkennen von Manipulation.
* **Qualität des Bildungssystems:** Kritisches Denken und Quellenbewertung sind Schlüsselkompetenzen.
* **Medienvielfalt:** Unabhängige Informationsquellen verhindern Monopole.

Die Kultusministerkonferenz hat bereits 2016 beschlossen, dass alle Schülerinnen und Schüler entsprechende Kompetenzen erwerben sollen. Doch auch Erwachsene müssen diese Kompetenzen entwickeln.

**Technologische und rechtliche Gegenmaßnahmen**
Parallel zur Bildung werden technologische Lösungen entwickelt:
* **KI gegen KI:** Algorithmen identifizieren verdächtige Muster und markieren Fake-Inhalte.
* **Faktencheck-Tools:** Automatisierte Systeme überprüfen Behauptungen in Echtzeit.
* **Transparenz-Initiativen:** Plattformen werden verpflichtet offenzulegen, wie ihre Algorithmen funktionieren und wer für politische Werbung bezahlt.

Auch die Politik wird aktiv, etwa durch Transparenzpflichten und Bot-Kennzeichnung (z. B. im Medienstaatsvertrag). Die Regulierung bleibt jedoch eine Herausforderung: Zu strenge Maßnahmen könnten die Meinungsfreiheit einschränken, zu schwache bieten keinen Schutz.

**Zivilgesellschaftliches Engagement**
Eine resiliente Gesellschaft braucht aktive Bürgerinnen und Bürger. Faktenchecking-Initiativen, Vereine für Medienbildung und Bürgerjournalismus spielen eine zentrale Rolle. Projekte zur digitalen Zivilcourage stärken das aktive Einschreiten gegen Hassrede.

**Was können Sie persönlich tun?**
* Überdenken Sie Ihr Informationsverhalten.
* Entwickeln Sie Ihre Medienkompetenz weiter.
* Zeigen Sie Dialogbereitschaft, auch mit Andersdenkenden.
* Stärken Sie die Zivilgesellschaft durch Unterstützung entsprechender Initiativen.

Der Kampf gegen Computational Propaganda ist ein Wettrüsten mit offenem Ausgang. Es ist eine gesamtgesellschaftliche Aufgabe. Wenn Bürgerinnen und Bürger, Bildungseinrichtungen, Medien, Politik und Technologieunternehmen an einem Strang ziehen, können wir eine resiliente, aufgeklärte Gesellschaft aufbauen.

---

## Transferaufgaben

Um das Gelernte in die Praxis zu übertragen, finden Sie hier Anregungen für Selbstreflexion und konkrete Übungen.

### Selbstreflexion
* **Eigene Informationsmuster analysieren:** Notieren Sie über eine Woche hinweg täglich Ihre fünf meistgenutzten Online-Quellen. Reflektieren Sie, welche Themen und Perspektiven darin dominieren und inwiefern Sie dadurch in einer Filterblase stecken könnten.
* **Emotionale Reaktionen protokollieren:** Erstellen Sie ein kurzes Tagebuch darüber, welche Beiträge starke Emotionen (Wut, Angst, Freude) ausgelöst haben. Fragen Sie sich: Warum haben gerade diese Inhalte Sie emotional gepackt, und könnte das gezieltes Emotional Targeting sein?
* **Vertrauens-Check:** Wählen Sie drei Artikel aus verschiedenen Quellen und prüfen Sie systematisch die Transparenz der Herkunft (Autor, Impressum, Quellen).
* **Eigene Medienkompetenz bilanzieren:** Welche Kenntnisse zu Bot-Erkennung und Deepfakes haben Sie bereits? Wo sehen Sie noch Lücken?

### Konkrete Transferaufgaben
* **News-Feed-Analyse:** Wählen Sie in Ihrem Social-Media-Feed drei Beiträge zu demselben Thema aus. Dokumentieren Sie Tonalität, Format und Herkunft. Vergleichen Sie dies mit einem Feed, dessen Nutzungsprofil Sie bewusst verändern.
* **Microtargeting-Experiment:** Legen Sie ein Zweitkonto mit gegensätzlichen Interessen an (z. B. Sport vs. Umwelt) und nutzen Sie es eine Woche parallel. Analysieren Sie die angezeigte Werbung und Posts auf psychografische Trigger.
* **Deepfake-Detektiv:** Suchen Sie ein aktuelles Deepfake-Beispiel, analysieren Sie typische Artefakte (unnatürliche Schatten, Asynchronität) und verfassen Sie einen kurzen Bericht.
* **Community-Resilienz-Konzept:** Entwickeln Sie ein Konzept für eine lokale Medienkompetenz-Initiative (z. B. Eltern-Infoabend).
* **Rollenspiel Medienrat:** Simulieren Sie eine Sitzung eines fiktiven Medienrats (Regulierungsbehörde, Plattformen, Zivilgesellschaft) und debattieren Sie über Bot-Kennzeichnung oder Transparenzpflichten.
* **Inoculation-Workshop entwerfen:** Entwerfen Sie ein Workshop-Modul, in dem Teilnehmende manipulierte Inhalte selbst entlarven („Impfung“ gegen Desinformation).
* **Ethik-Dilemma-Debatte:** Recherchieren Sie einen realen Fall von KI-Propaganda und diskutieren Sie die ethischen Verantwortlichkeiten von Plattformen, Politik und Nutzenden.

## Zusammenfassung

Sie verfügen nun über ein solides Verständnis der zentralen Instrumente digitaler Manipulation und haben praxisorientierte Analyse- und Erkennungsmethoden kennengelernt: Sie können Bot-Netzwerke und Deepfakes identifizieren, Filterblasen und Echokammern thematisieren sowie Mikrotargeting erkennen.

Sie sind in der Lage, in Workshops und Schulungsformaten medienpädagogische Konzepte zur Stärkung der Resilienz umzusetzen, reflektierte Diskussionsrunden zu leiten und schulische Digitalstrategien zu entwickeln, die Ihr Kollegium sowie Ihre Schülerinnen und Schüler für Desinformation sensibilisieren. Nutzen Sie die bereitgestellten Ressourcen, um langfristig eine aufgeklärte und kritische Medienkultur in Ihrer Einrichtung zu etablieren.