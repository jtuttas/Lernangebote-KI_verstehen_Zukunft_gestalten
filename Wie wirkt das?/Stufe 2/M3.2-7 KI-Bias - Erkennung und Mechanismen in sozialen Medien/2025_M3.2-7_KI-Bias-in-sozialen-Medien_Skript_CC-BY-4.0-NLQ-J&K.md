Hier ist der extrahierte Inhalt des Lernangebots, aufbereitet als lesbarer Fließtext ohne Metadaten und Regieanweisungen.

# M3.2-7 – KI-Bias: Erkennung und Mechanismen in sozialen Medien

## Start: Worum geht es?

Dieses Lernangebot widmet sich dem Thema KI-Bias in sozialen Medien und beleuchtet, wie algorithmische Verzerrungen entstehen, wirken und erkannt werden können. Im Mittelpunkt steht die Frage, wie künstliche Intelligenz durch unausgewogene Trainingsdaten gesellschaftliche Ungleichheiten verstärkt und welche Verantwortung sich daraus für uns alle ergibt.

Anhand von vier aufeinander aufbauenden Videos werden zentrale Konzepte wie Input-Bias, verzerrte Trainingsdaten, algorithmische Meinungsbildung und Bias-Audits anschaulich erklärt. Dabei wird gezeigt, wie subtile Formen von Diskriminierung in sozialen Medien entstehen, warum sie schwer zu erkennen sind und welche Strategien helfen, diesen Mechanismen bewusst zu begegnen.

Das Lernangebot bietet praxisorientierte Beispiele aus der Social Media-Nutzung, ergänzt durch konkrete Handlungsempfehlungen für Unterricht und Verwaltung. Lehrkräfte und pädagogisch Verantwortliche erfahren, wie sie Jugendliche dabei unterstützen können, algorithmische Verzerrungen zu erkennen und zu verstehen, ihre Mediennutzung kritisch zu reflektieren und digitale Vielfalt zu fördern.

Nach Abschluss können Sie erklären, was unter KI-Bias zu verstehen ist, wie Input-Bias in sozialen Medien entsteht und welche Maßnahmen zur Prävention und Aufklärung beitragen.

Dieses Lernangebot richtet sich an alle Personen, die die Einflüsse künstlicher Intelligenz auf Meinungsbildung und gesellschaftliche Wahrnehmung besser erkennen, verstehen und reflektiert in ihre Arbeit integrieren möchten.

Das Lernangebot umfasst vier Videos mit ergänzenden Praxistipps, Reflexionsaufgaben, Transferaufgaben und vertiefenden Materialien.

---

## Testen Sie Ihr Vorwissen

**Frage 1: Was beschreibt der Begriff Input-Bias?**
* Eine Verzerrung, die durch falsche Programmierung von Algorithmen entsteht.
* *Eine Verzerrung, die durch unausgewogene oder unvollständige Trainingsdaten verursacht wird.*
* Eine technische Fehlfunktion bei der Dateneingabe.
* Eine absichtliche Manipulation von Suchergebnissen.

**Frage 2: Welche Folge kann Input-Bias in sozialen Medien haben?**
* Eine zufällige Verteilung der Inhalte in allen Feeds.
* Eine gleichmäßige Darstellung unterschiedlicher Perspektiven.
* *Eine einseitige Informationsverteilung und die Verstärkung bestehender Vorurteile.*
* Eine Verbesserung der Datenqualität durch Nutzerfeedback.

**Frage 3: Was ist eine wesentliche Ursache für Input-Bias?**
* Fehlende Rechenleistung der KI-Systeme.
* *Überrepräsentation bestimmter Gruppen oder Themen in den Trainingsdaten.*
* Zu hohe Transparenz in der Datenauswahl.
* Zufällige Schwankungen in der Datenmenge.

**Frage 4: Warum werden Dialekte in der Spracherkennung oft falsch verstanden?**
* *Weil sie nicht standardisiert sind und zu wenig in den Trainingsdaten vorkommen.*
* Weil die Systeme auf zu vielen Sprachen gleichzeitig trainiert wurden.
* Weil Dialekte für KI grundsätzlich nicht erkennbar sind.
* Weil Dialektsprecher ihre Stimme zu leise aufnehmen.

**Frage 5: Was versteht man unter Bias-Audit?**
* *Eine regelmäßige Überprüfung von KI-Systemen auf mögliche Verzerrungen.*
* Eine technische Wartung der Hardwarekomponenten von KI-Systemen.
* Eine Schulung für Lehrkräfte zur Nutzung von Social Media.
* Ein automatisches Feedbacksystem für Social Media-Algorithmen.

**Frage 6: Welche Gruppen sind in Social Media-Daten häufig unterrepräsentiert?**
* Jüngere, technikaffine Personen.
* Menschen mit höherem Einkommen.
* *Ältere Menschen oder Personen mit eingeschränktem Zugang zu digitalen Geräten.*
* Professionelle Influencer.

**Frage 7: Welche Maßnahme trägt zur Prävention von Input-Bias bei?**
* Nutzung homogener Datensätze zur Vereinheitlichung der Ergebnisse.
* Verzicht auf externe Datenquellen.
* *Sammlung vielfältiger und repräsentativer Trainingsdaten.*
* Beschränkung der Datenauswahl auf bestimmte Altersgruppen.

**Frage 8: Was ist ein Beispiel für algorithmische Diskriminierung in sozialen Medien?**
* *Automatische Aktivierung von Beauty-Filtern, die ein einseitiges Schönheitsideal fördern.*
* Zufällige Anzeige von unterschiedlichen Inhalten im Feed.
* Gleichmäßige Verteilung von Beiträgen über alle Nutzergruppen.
* Bevorzugung sachlicher Inhalte gegenüber emotionalen Beiträgen.

**Frage 9: Welche Rolle spielt Transparenz im Umgang mit KI-Systemen?**
* *Sie ermöglicht es, algorithmische Entscheidungen zu verstehen und zu hinterfragen.*
* Sie macht KI-Systeme automatisch fairer.
* Sie ersetzt die Notwendigkeit regelmäßiger Datenüberprüfung.
* Sie verhindert die Weiterentwicklung von Algorithmen.

**Frage 10: Welches Ziel verfolgt die Bildungsarbeit im Zusammenhang mit KI-Bias?**
* Jugendliche sollen lernen, soziale Medien zu meiden.
* *Jugendliche sollen verstehen, wie KI funktioniert und kritisch damit umgehen.*
* Lehrkräfte sollen Social Media-Profile der Lernenden kontrollieren.
* Schulen sollen auf KI-Technologien verzichten.

**Auswertung:**
* *Einstieg (0-3 richtige Antworten):* Klasse, dass Sie sich an den Fragen versucht haben! Jetzt können Sie tiefer in die Materie eintauchen.
* *Fortgeschritten (4-7 richtige Antworten):* Sie haben schon ein solides Grundwissen. Der Kurs wird Ihnen helfen, Themen weiter zu vertiefen.
* *Profi (8-10 richtige Antworten):* Beeindruckend! Ihr Wissen ist bereits fundiert. Der Kurs kann Ihnen eine Auffrischung und neue Impulse bieten.

---

## Grundlagen

**Bevor es los geht …**

Dieser Kurs richtet sich an Personen, die verstehen möchten, wie künstliche Intelligenz in sozialen Medien wirkt und welche Rolle algorithmische Verzerrungen bei der Beeinflussung von Wahrnehmung und Meinungsbildung spielen.

Ziel ist es, die Entstehung von Input-Bias und anderen Formen von KI-Bias zu verstehen, Risiken kritisch einzuordnen und pädagogische Strategien zu entwickeln, um Jugendliche zu einem reflektierten und verantwortungsvollen Umgang mit digitalen Medien zu befähigen.

**Behandelte Aspekte im Überblick:**
* **Verzerrungen verstehen:** Ursachen und Formen von Input-Bias in sozialen Medien analysieren
* **Daten kritisch bewerten:** Repräsentativität, Vielfalt und Transparenz von Trainingsdaten prüfen
* **Wirkungen erkennen:** Einfluss algorithmischer Entscheidungen auf Wahrnehmung und Verhalten reflektieren
* **Pädagogisch handeln:** Strategien zur Förderung von Medienkompetenz und digitaler Vielfalt entwickeln

Mit diesem Modul legen Sie die Grundlage, um KI-gestützte Medienumgebungen im Unterricht und in schulischen Prozessen kritisch zu begleiten, Bewusstsein für algorithmische Einflussnahme zu schaffen und faire sowie inklusive Lernkontexte zu fördern.

---

### Video 1: Einführung – Was bedeutet KI-Bias?

Stellen Sie sich vor, eine Lernplattform schlägt einer Gruppe von Schülerinnen und Schülern nach drei Klicks immer dieselben Aufgabentypen vor, obwohl ihre Interessen deutlich variieren. Der Algorithmus erkennt ein vermeintliches Muster in den wenigen Interaktionen und verstärkt dieses anschließend bei jeder weiteren Empfehlung. Wer anfangs zu einem bestimmten Thema klickt, erhält künftig fast ausschließlich Inhalte aus genau diesem Bereich, während andere Perspektiven unsichtbar bleiben. So entsteht eine Verzerrung, die Lernwege verengt und Chancen auf breite Kompetenzentwicklung mindert.

Ein vergleichbarer Effekt zeigt sich in sozialen Medien: Wenn Lernende in einer Kurzvideo-Plattform ein einziges Video zu einem kontroversen Thema positiv bewerten, folgt die Empfehlungslogik demselben Verstärkungsprinzip. Innerhalb kürzester Zeit dominiert dann ein enges Meinungsspektrum den persönlichen Feed, während abweichende Sichtweisen kaum noch auftauchen. Diese Filterblasen können bestehende Stereotype verstärken, politische Diskussionen einseitig prägen und das Weltbild der Beteiligten erheblich verzerren.

Diese Verzerrung ist ein Beispiel für Bias in Systemen der künstlichen Intelligenz. Das Bundesamt für Sicherheit in der Informationstechnik beschreibt Bias als systematische Abweichung zwischen den tatsächlichen Eigenschaften von Personen oder Sachverhalten und der Art, wie ein Algorithmus sie abbildet. Bias kann entstehen, weil Trainingsdaten einseitig sind, weil Modelle fehlerhafte Annahmen verankern oder weil Nutzungsumgebungen bestimmte Gruppen benachteiligen.

Auf der Datenebene spricht man von Eingabeverzerrung, wenn die gesammelten Informationen nicht die Vielfalt der späteren Anwendungssituation widerspiegeln. Die Nutzungsebene betrifft die Rückkopplung zwischen Algorithmus und Unterrichts- oder Arbeitsrealität: Wenn eine verzerrte Empfehlung das Lern- oder Arbeitsverhalten beeinflusst, kehren genau diese verzerrten Ergebnisse als neues Trainingsmaterial zurück und verschärfen das Problem weiter.

Warum ist das für Bildungseinrichtungen bedeutsam? Verzerrte Algorithmen können Lernende systematisch benachteiligen, etwa weil bestimmte Interessen, Sprachniveaus oder kulturelle Hintergründe in den Daten unterrepräsentiert sind. Ebenso beeinflussen sie Verwaltungsprozesse: Die Klassifikation von Text kann Eingaben von Eltern mit Migrationshintergrund falsch priorisieren, wenn Anfragen in einfacher Sprache im Trainingsmaterial kaum vorkommen. Ein automatisches Zuweisungssystem für Schulplätze kann Bewerbungen aus bestimmten Postleitzahlgebieten benachteiligen, wenn historische Daten Ungleichverteilungen enthalten. Selbst ein Chatbot im Sekretariat kann Nachfragen zu Nachteilsausgleichen übersehen, weil entsprechende Formulierungen im Trainingsmaterial fehlen.

Darüber hinaus verbringen Schülerinnen und Schüler einen großen Teil ihrer Freizeit in sozialen Medien, deren Empfehlungslogik dieselbe Verzerrungsdynamik aufweist: Wer zweimal auf irreführende oder sensationsorientierte Inhalte klickt, erhält eine Flut gleichartiger Beiträge. Dadurch können Stereotype verfestigt, Falschinformationen verbreitet und gruppenbezogene Feindbilder gestärkt werden.

Lehrkräfte und die Schulverwaltung müssen dieses Zusammenspiel verstehen, weil schulische Medienerziehung nur dann wirksam ist, wenn sie die digitale Alltagswelt der Lernenden einbezieht. Mit diesem Hintergrundwissen können sie Lernende zu reflektiertem Mediengebrauch anleiten, problematische Trends früh erkennen und in schulischen Projekten Gegenstrategien wie Faktenchecks, vielfältige Quellenarbeit und Perspektivwechsel fördern.

Die Empfehlung der Organisation der Vereinten Nationen für Erziehung, Wissenschaft und Kultur zur Ethik der künstlichen Intelligenz fordert deshalb Transparenz, Fairness und menschliche Aufsicht in allen Bildungsszenarien. Auch aus politischer Sicht wächst der Druck, verzerrte Systeme zu erkennen und zu korrigieren. Die Organisation für wirtschaftliche Zusammenarbeit und Entwicklung empfiehlt Deutschland, in Schulen und anderen Bildungseinrichtungen regelmäßige Bias-Audits einzuführen, um Chancengleichheit zu sichern und Vertrauen in digitale Lösungen zu stärken.

Zusammengefasst liegt Bias nicht nur in fehlerhaften Daten, sondern in einem komplexen Zusammenspiel aus Datenauswahl, Modellarchitektur und Nutzungskontext. Wer künstliche Intelligenz in der Bildung verantwortungsvoll einsetzen möchte, muss diese Wechselwirkungen verstehen, Qualitätskriterien für Daten anwenden und den laufenden Betrieb fortwährend prüfen. Nur so lassen sich verzerrte Lernpfade vermeiden und die Potenziale digitaler Unterstützung wirklich ausschöpfen.

---

### Video 2: Input-Bias, Datenqualität und Gegenmaßnahmen

Stellen Sie sich vor, ein automatisches Bewertungssystem für Aufsätze vergibt systematisch niedrigere Punkte an kreative Texte, weil die Trainingsbeispiele überwiegend formelhafte Antwortstrukturen enthielten. Eine Lernende mit ungewöhnlichen Stilmitteln erreicht dadurch eine geringere Note und verliert Motivation. Gleichzeitig priorisiert ein Verwaltungsprogramm Förderangebote für Lernende aus Stadtteilen mit hoher Teilnahmequote, weil historische Daten diese Gruppe besonders stark abbilden. Auf sozialen Medien genügt es, wenn Jugendliche zweimal ein Fitnessvideo positiv markieren, damit ihr Feed anschließend fast ausschließlich Beiträge mit extremen Körperidealen zeigt. In allen drei Fällen entstehen Verzerrungen, die Lernwege verengen und Weltbilder einseitig formen.

Diese Beobachtungen verweisen auf systematische Verzerrungen in Systemen der künstlichen Intelligenz. Verzerrungen können sich auf mehreren Ebenen bilden:

Erstens liegt eine Eingabeverzerrung vor, wenn die Trainingsdaten nicht die Vielfalt der späteren Anwendung repräsentieren. Fehlen zum Beispiel Texte von Lernenden mit Deutsch als Zweitsprache, bewertet das Modell deren Ausdrucksweise schlechter und übersieht passende Förderempfehlungen.

Zweitens entsteht eine Verzerrung, wenn die Kategorien oder Bewertungen selbst bereits voreingenommen sind, etwa weil frühere Aufsatznoten kreative Stilmittel systematisch strenger beurteilt haben. Das Modell übernimmt diese Bewertungen als objektive Vorgabe und reproduziert die Unausgewogenheit.

Drittens entwickelt sich eine Modellverzerrung, wenn die verborgenen Schichten, die sogenannten Hidden Layers, statistische Muster verstärken, die zwar mathematisch bedeutsam, jedoch pädagogisch wenig sinnvoll sind. So können Merkmale wie Satzlänge oder Dialekt stärker gewichtet werden als inhaltliche Qualität.

Viertens kommt es zu einer Rückkopplungsverzerrung, wenn ein System fortlaufend Daten aus seinen eigenen Vorhersagen bezieht. Ein erster Irrtum verfestigt sich und jede weitere Iteration vergrößert den Abstand zwischen bevorzugten und benachteiligten Gruppen.

Fünftens entsteht eine Kontextverzerrung, wenn der Einsatzrahmen das Nutzerverhalten lenkt. Wird eine Lernapp ausschließlich in Pausenzeiten genutzt, wählen Schülerinnen und Schüler überwiegend vertraute Themen und das System interpretiert diese Auswahl als dauerhafte Vorliebe.

Sechstens tritt eine Präsentationsverzerrung auf, wenn die Darstellung der Ergebnisse das weitere Handeln beeinflusst. Werden Lehrkräften Übersichten nach Ampelfarben angezeigt, erhalten Aufsätze mit gelbem Symbol weniger Aufmerksamkeit, selbst wenn das Leistungspotenzial hoch ist.

Die Hidden Layers sind die Schichten im Inneren eines neuronalen Netzes, in denen das Modell aus den Eingabedaten Schritt für Schritt abstrakte Merkmale bildet. Jede Schicht betont bestimmte Eigenschaften und schwächt andere ab. Wenn ein System in seinen verborgenen Schichten zum Beispiel Sprachelemente überbewertet, die in einer bestimmten Region häufig vorkommen, kann es Lernende aus anderen Regionen benachteiligen. Ebenso kann in der Schulverwaltung ein Zuweisungsmodell unbewusst Anträge priorisieren, die bestimmte formale Floskeln enthalten, wodurch Eltern mit einem schlichteren Sprachstil benachteiligt werden. In sozialen Medien wiederum kann ein Empfehlungsalgorithmus in seinen Hidden Layers Reaktionsmuster auf reißerische Schlagwörter stärker gewichten und so Beiträge mit extremen Positionen bevorzugen.

Da Lehrkräfte und Verwaltungsmitarbeitende diese Gewichtungen nicht direkt einsehen können, sind Transparenz über die Trainingsdaten und Werkzeuge zur Analyse der Schichten unverzichtbar, um Verzerrungen zu erkennen.

Die folgenden praktischen Beispiele veranschaulichen, welche Folgen das haben kann. Im Unterricht führt die beschriebene Eingabeverzerrung dazu, dass leistungsstarke Klassen überschätzt und schwächere Klassen unterschätzt werden, weil das System überwiegend Daten aus Gymnasien verarbeitet hat. In der Schulverwaltung sortiert ein automatischer Ticketrouter Anfragen von Eltern aus einkommensschwächeren Postleitzahlen in Wartebereiche mit niedriger Priorität, weil historische Daten längere Antwortzeiten aufweisen. In der Freizeit werden Schülerinnen und Schüler durch einseitige Kurzvideo-Feeds auf bestimmte gesellschaftliche Sichtweisen festgelegt, was politische Gräben vertieft.

Es gibt alltagspraktische Gegenstrategien, die ohne eigenes Modelltraining auskommen. Erstens können Lehrkräfte und Verwaltungsmitarbeitende Filter vor der Dateneingabe einsetzen: Aufgabentexte, Formulare oder Chatbot‑Prompts werden so gestaltet, dass sie unterschiedliche Perspektiven anregen, und sensible Merkmale wie Herkunft oder Geschlecht werden bewusst neutral kodiert.

Dazu ein Beispiel:
Ausgangsprompt: „Erstelle eine Zeugnisbemerkung für Hassan, Hauptschule, unteres Leistungsniveau, wenig Motivation.“
Probleme: Der Vorname wird mit einer bestimmten Herkunft assoziiert. Schulform und Leistungsniveau werden im Prompt direkt gekoppelt und können zu negativen Stereotypen führen.
Überarbeiteter Prompt: „Erstelle eine respektvolle, motivierende Zeugnisbemerkung für eine Lernende oder einen Lernenden. Die Person zeigt vereinzelt Verständnislücken in Mathematik, hat Fortschritte im Fach Deutsch gemacht und bringt sich aktiv im Klassenrat ein.“

Zweitens helfen Kontrollfilter nach der Ausgabe, etwa Stichproben, bei denen Notenvorschläge oder Ticketpriorisierungen automatisiert auf auffällige Abweichungen geprüft werden. Zurück zum vorigen Beispiel. Ein Filter nach der Ausgabe wäre folgendes Szenario: Prüfen Sie, ob die generierte Bemerkung Formulierungen wie „schwach“ oder „wenig motiviert“ enthält. Falls ja, ersetzen Sie diese durch konstruktive Hinweise, zum Beispiel „braucht noch Unterstützung beim Verfestigen grundlegender Rechenstrategien“.

Drittens sollten Schulen und Ämter halbjährliche Bias‑Audits durchführen: Mit frei verfügbaren Prüftabellen, zum Beispiel aus dem Whitepaper des Bundesamtes für Sicherheit in der Informationstechnik, erheben sie Kennzahlen wie durchschnittliche Bearbeitungszeit pro Postleitzahl oder Notenverteilung nach Sprachhintergrund und gleichen sie mit Zielwerten ab.

Viertens erleichtern Ampeldashboards den Alltag: Starke Abweichungen werden sofort farblich markiert, sodass Lehrkräfte oder Sachbearbeitende die Kriterien anpassen können.

Fünftens sind Feedbackschleifen mit Lernenden wertvoll; kurze Reflexionsphasen erfassen, ob Empfehlungen oder Feeds als einseitig empfunden werden, und speisen diese Erkenntnisse in die Auditprüfung ein. Wo Entscheidungen große Konsequenzen haben, etwa bei Versetzungsempfehlungen oder der Vergabe von Fördermitteln, bleibt eine abschließende menschliche Prüfung unerlässlich.

Wer diese Schritte beherzigt, verringert Verzerrungen im Unterricht, in der Schulverwaltung und in der digitalen Alltagswelt der Lernenden. So lassen sich Lernchancen gerechter verteilen und die Potenziale der künstlichen Intelligenz verantwortungsvoll nutzen.

---

### Checkliste für Prompt-Engineering

Beachten Sie: Die Nutzung von KI-Tools in Ihrem Arbeitsalltag ist im Vorfeld vom Datenschutzbeauftragten zu prüfen. Grundsätzlich dürfen keine personenbezogenen oder personenbeziehbaren Daten in KI eingegeben werden.

Nutzen Sie diese Checkliste in der folgenden Zeit bei Ihren KI-Anfragen und werten Sie Ihre Ergebnisse aus. Die Checkliste soll Sie dabei unterstützen, ein Bewusstsein für die Wirkungen Ihrer Eingaben zu bekommen. Im zweiten Schritt können Sie erproben, wie sie ähnliche Aspekte in Ihre Social Media Nutzung integrieren können.

* **Absicht klären:** Formulieren Sie in einem Satz, welches Ergebnis die künstliche Intelligenz liefern soll und welches Problem Sie damit lösen möchten.
* **Perspektivenvielfalt sicherstellen:** Sammeln Sie vorab unterschiedliche Blickwinkel, zum Beispiel regionale Varianten, Altersspannen, Geschlechterrollen oder fachliche Hintergründe, und beziehen Sie diese ausdrücklich in den Prompt ein.
* **Sensible Merkmale prüfen:** Überlegen Sie, ob Merkmale wie Herkunft, soziale Schicht oder gesundheitlicher Status im Ergebnis relevant sein dürfen, und benennen Sie nur diejenigen Merkmale, die für die Aufgabe notwendig sind.
* **Kontext eindeutig beschreiben:** Geben Sie Rahmenbedingungen, Zielgruppe und gewünschtes Format an. Ein klarer Kontext verringert Fehlinterpretationen und verdeckte Verzerrungen.
* **Sprache variieren:** Nutzen Sie unterschiedliche Formulierungen und Sprachstile, um zu testen, ob das System bestimmte Ausdrucksweisen bevorzugt oder benachteiligt.
* **Anweisung zur Fairness einbauen:** Ergänzen Sie Formulierungen wie „Achte auf ausgewogene Darstellung“ oder „Vermeide stereotype Zuordnungen“, damit das Modell die Erwartung nachvollziehen kann.
* **Zwischenergebnisse anfordern:** Bitten Sie um eine kurze Gliederung oder Stichwortliste, bevor der vollständige Text erzeugt wird, um frühzeitig Verzerrungen erkennen zu können.
* **Kontrollfrage hinzufügen:** Lassen Sie das System sein eigenes Ergebnis kritisch prüfen, zum Beispiel mit der Anweisung „Bewerte das Ergebnis auf mögliche Einseitigkeit“.
* **Antwort reflektieren und nachbearbeiten:** Lesen Sie das Resultat bewusst quer, markieren Sie auffällige Passagen und passen Sie diese gegebenenfalls manuell an.
* **Dokumentation führen:** Notieren Sie jeden Prompt, das Datum und Ihre Beobachtungen zu Verzerrungen. Eine kontinuierliche Dokumentation erleichtert spätere Audits und Verbesserungen.

---

### Video 3: „Meinungen“ und Mechanismen in sozialen Medien

Stellen Sie sich vor, Lernende öffnen eine Kurzvideo Plattform nach dem Unterricht. Nachdem sie zwei satirische Clips mit herabwürdigenden Stereotypen positiv bewertet haben, füllt sich ihr persönlicher Feed fast vollständig mit ähnlich gefärbten Beiträgen. Kurz darauf erscheinen Werbevideos für fragwürdige Diätpillen, weil der Algorithmus aus denselben Interaktionen ein starkes Interesse an Schönheitsidealen ableitet. In einem anderen Netzwerk löst eine einzige geteilte Überschrift zu einem politischen Thema eine Flut von stark emotionalisierten Nachrichten aus, während differenzierte Analysen kaum mehr angezeigt werden. Auf einer Community‑Plattform wiederum entfernt ein automatisches Moderationsmodell vermehrt Posts in migrantischen Dialekten, weil bestimmte Ausdrücke fälschlich als aggressiv eingestuft werden, während codierte Hassrede in Standardsprache häufig durchrutscht. Diese Beispiele aus sozialen Medien verdeutlichen, dass künstliche Intelligenz nicht nur aufgrund der Datenlage verzerrt, sondern auch wegen ihrer inneren Gewichtungen, ihrer versteckten „Meinungen“.

Jedes moderne KI‑System besteht aus einer Eingabeschicht, vielen verborgenen Schichten und einer Ausgabeschicht. In den verborgenen Schichten entstehen während des Trainings abstrakte Merkmalskombinationen. Das Modell lernt zum Beispiel, kurze Sätze mit hoher Dynamik in Videoüberschriften als spannend einzuordnen. Solche Gewichtungen beruhen auf Häufigkeiten in den Trainingsdaten, ersetzen aber oft eine bewusste Qualitätsbeurteilung. Sie können sich verselbstständigen; ein scheinbar neutraler Empfehlungsalgorithmus einer Kurzvideo Plattform ist dann nicht mehr objektiv, weil er kurze, provokante Schlagzeilen höher gewichtet und differenzierte Beiträge ausblendet.

Für Nutzende von Social Media Plattformen ist es essenziell eine Sensibilität für Bias und Meinungen des System zu entwickeln. Erstens brauchen sie Anhaltspunkte dafür, wann algorithmische Vorschläge zu einseitig erscheinen. Ein einfacher Hinweis ist, wenn der Feed fast nur noch identische Kurzvideos oder Posts enthält, immer dieselben Creator vorgeschlagen werden oder Trending‑Challenges ausschließlich eine Sichtweise zeigen.

Zweitens werden alltagspraktische Eingriffsmöglichkeiten benötigt. Es können bewusst Accounts aus unterschiedlichen Regionen, Kulturen und politischen Spektren abonniert und so der Algorithmus mit Vielfalt gefüttert werden. Während und nach dem Konsum helfen Feedbackschleifen: Was waren heute die Inhalte meines Feeds? War die Meinung einseitig? Welche Stimmen fehlen? Es braucht außerdem ein Wissen über Einstellungen wie „Interessen zurücksetzen“ oder darüber, dass regelmäßiges Löschen des Verlaufs die Vielfalt der Inhalte erhöhen kann. Und nicht zuletzt braucht es den Austausch außerhalb der eigenen Filterblase, um ein Gefühl für die Vielfalt an Meinungen zu erlangen und einseitige Informationen entsprechend als solche entlarven zu können.

Am Ende bleibt festzuhalten: Die inneren „Meinungen“ einer KI beruhen auf statistischen Verkürzungen, nicht auf reflektierten Werturteilen. Lehrkräfte und Mitarbeitende der Schulverwaltung können die Algorithmen sozialer Medien zwar nicht direkt verändern, aber sie besitzen wirksame Hebel, um deren Einfluss einzuordnen. Erstens fördern sie Aufklärung und Transparenz, indem sie mit Lernenden regelmäßig Feeds analysieren und dabei verdeckte Gewichtungen sichtbar machen. Zweitens regen sie Perspektivwechsel an, etwa durch das bewusste Abonnieren kontrastierender Kanäle oder das Einführen digitaler Fastentage, an denen alternative Informationsquellen genutzt werden. Drittens richten sie einfache Bias‑Audits ein: Mehrmals im Jahr werden die Startseiten von Testaccounts unterschiedlicher Profiltypen verglichen, Abweichungen dokumentiert und die Ergebnisse im Kollegium diskutiert. So wird Medienbildung zu einem kontinuierlichen Prozess, der einseitige Inhalte eindämmt und die Fähigkeit der Lernenden stärkt, algorithmische Verzerrungen zu erkennen.

---

### Feedanalyse mit Lernenden (oder des persönlichen Feeds)

* Teilen Sie die Lernenden in Zweiergruppen ein. Jede Gruppe legt zwei neue Testkonten auf derselben Social Media Plattform an.
* Konto A folgt nur Nachrichtenanbietern, Konto B nur Unterhaltungsprofilen.
* Sammeln Sie drei Tage lang jeweils zur gleichen Uhrzeit Bildschirmfotos der Startseite beider Konten.
* Übertragen Sie die Beiträge in eine Tabelle mit den Spalten Uhrzeit, Quelle, Thema, Ton, Haltung.
* Markieren Sie jede Zeile mit einer Ampelfarbe: Grün für vielfältige Darstellung, Gelb für leichte Einseitigkeit, Rot für starke Einseitigkeit oder emotional gefärbte Sprache.
* Vergleichen Sie die Farbmuster der beiden Konten. Diskutieren Sie im Plenum, wie Algorithmen Vorlieben verstärken und welche Auswirkungen eine aktive Quellenwahl hat.
* Leiten Sie gemeinsam Regeln für einen ausgewogenen Feed ab, zum Beispiel ausgewählte Nachrichtenquellen abonnieren, thematische Listen anlegen oder Zeitlimits festlegen.

---

### Video 4: Bias-Audits in der Praxis

Stellen Sie sich vor, eine Schule erhält Beschwerden, weil ein Textgenerator Zeugnisvorschläge formuliert, in denen Schülerinnen und Schüler aus bestimmten Stadtteilen häufiger als leistungsschwach beschrieben werden. Gleichzeitig meldet das Sekretariat, dass ein automatisches Ticket System Anfragen von Eltern in einfacher Sprache seltener priorisiert. Beide Fälle weisen auf mögliche Diskriminierung hin und machen deutlich, warum Bildungseinrichtungen in Deutschland eine Pflicht zur Prüfung von Algorithmen haben. Der Deutsche Bundestag und Landesdatenschutzbeauftragte fordern regelmäßig den Nachweis, dass digitale Prozesse keine Gruppen benachteiligen. Auch die Datenschutz-Grundverordnung (DSGVO, Artikel 5 und 22) verpflichtet Verantwortliche dazu, automatisierte Entscheidungen transparent zu gestalten und unzulässige Diskriminierungen auszuschließen.

Ein Bias Audit ist ein geplanter Prüfprozess, der genau diese Pflicht erfüllt. Er folgt drei Schritten. Erstens legt die Einrichtung konkrete Fairness Ziele fest. Dazu gehört zum Beispiel die Vorgabe, dass Vorschläge für Zeugnisformulierungen sowie Antwortzeiten im Sekretariat unabhängig von Geschlecht, Wohnort oder sprachlichem Hintergrund gleich verteilt sein sollen.

Zweitens testet ein Team das System mit sogenannten Balance Datensätzen. Diese Datensätze enthalten bewusst gleiche Anteile aller relevanten Gruppen, zum Beispiel Aufgabenlösungen von leistungsstarken und leistungsschwachen Klassen oder Anfragen in Standarddeutsch und in einfacher Sprache. Vergleicht man die Ergebnisse, erkennt man schnell, ob das System bestimmte Gruppen bevorzugt oder benachteiligt.

Drittens passt man Einstellungen, Trainingsdaten oder Regelwerke an und testet das Ergebnis erneut, bis die festgelegten Fairness Kriterien erfüllt sind.

Leitlinien und Frameworks erleichtern den Einstieg. Das Whitepaper des Bundesamtes für Sicherheit in der Informationstechnik bietet Checklisten für Datenqualität, Modellbewertung und Ergebnisüberwachung. Der Bericht der Organisation für wirtschaftliche Zusammenarbeit und Entwicklung zu Deutschland empfiehlt darüber hinaus, Audit Ergebnisse jährlich in einem Transparenzbericht zu veröffentlichen, um das Vertrauen von Lehrkräften, Eltern und Lernenden zu stärken.

Für die Praxis hilft ein Blick auf typische Beispiele. Im Unterricht können Lehrkräfte einen Textgenerator prüfen, der Zeugnis Vorschläge erstellt: Sie sammeln Formulierungen für Lernende unterschiedlicher Leistungsniveaus und prüfen, ob bestimmte Adjektive wie engagiert oder unsicher einseitig verteilt sind. In der Verwaltung lässt sich ein Ticketrouter auditieren, indem man Anfragen in einfacher Sprache und in Fachsprache vergleicht und die gemessenen Antwortzeiten gegenüberstellt.

Bias Audits bringen klare Vorteile. Nachweisbare Fairness steigert das Vertrauen aller Beteiligten und verbessert die Akzeptanz digitaler Hilfsmittel. Gleichzeitig erfordert der Prozess Zeit, Fachwissen und meist eine interdisziplinäre Arbeitsgruppe. Einheitliche Standards fehlen allerdings noch, sodass Einrichtungen eigene Bewertungsmaßstäbe entwickeln müssen. Wer ein Bias-Audit-Zyklus konsequent durchführt, reduziert Diskriminierungsrisiken, erhöht die Qualität digitaler Prozesse und schafft eine Grundlage für verantwortungsvolle Innovation.

---

### Input-Bias-Check mit Ampeldashboard

Erproben Sie ein Audit-Verfahren, um die Prinzipien zu verinnerlichen und die Erkenntnisse bei Ihrer KI-Nutzung stets mitzudenken.

* Öffnen Sie ein neues Tabellenblatt mit den Spalten Prompt, Antwort, Ampelfarbe, Notizen.
* Sammeln Sie zehn möglichst unterschiedliche Prompts, die dasselbe Thema aus verschiedenen Perspektiven beleuchten.
* Achten Sie bewusst auf Variationen bei Sprachstil, Dialekt, Geschlecht, Alter sowie fachlichem Vorwissen.
* Lassen Sie das genutzte System der künstlichen Intelligenz alle Prompts beantworten und tragen Sie die Ergebnisse ein.
* Bewerten Sie jede Antwort:
    * **Grün** = keine erkennbaren Verzerrungen
    * **Gelb** = leichte Tendenz zu einseitigen Darstellungen
    * **Rot** = deutliche Verzerrung oder diskriminierende Formulierungen
* Prüfen Sie das Muster der Ampelfarben. Bei mehreren roten oder gelben Feldern analysieren Sie, welche Merkmalsgruppen schlechter bedient werden und passen Sie Prompts, Filter oder Nachbearbeitung entsprechend an.
* Wiederholen Sie den Test regelmäßig und bezogen auf Ihren Arbeitsalltag.
* Ergänzen Sie neue Prompts, um Veränderungen im Systemverhalten im Blick zu behalten.

---

## Vertiefung

**Vertiefende Informationen zum Thema**

* Das Whitepaper „Bias in der Künstlichen Intelligenz“ bietet praxisnahe Checklisten für Datenqualität, Modellbewertung und Ergebnisüberwachung, die sich unmittelbar für eigene Schul- oder Verwaltungs-Audits übernehmen lassen.
* Die „Empfehlung zur Ethik Künstlicher Intelligenz” der UNESCO liefert zentrale Leitprinzipien zu Transparenz, Fairness und Aufsicht, mit denen Lehrkräfte Verzerrungen nicht nur erkennen, sondern auch ethisch einordnen können.
* Der OECD-Bericht zu Künstlicher Intelligenz in Deutschland beschreibt konkrete Handlungsempfehlungen für Schulen und Verwaltung, um Bias-Audits als jährliche Routine zu verankern.
* „Künstliche Intelligenz in der Schule. Eine Handreichung zum Stand in Wissenschaft und Praxis“ fasst empirische Befunde zu Input-Bias in Lernplattformen zusammen und zeigt, welche Datenlücken besonders häufig zu Benachteiligungen führen.
* „BILDUNG UND KI“ definiert Bias und dessen Entstehung durch menschliche Vorurteile und unzureichende Trainingsdaten und thematisiert die gravierenden Risiken im Bildungsbereich, inklusive notwendiger regulatorischer Maßnahmen.
* Der Bericht „Zum aktuellen Einsatz von Künstlicher Intelligenz in Bildungseinrichtungen“ des Deutschen Bundestags erläutert rechtliche Pflichten, etwa Diskriminierungs- und Datenschutzanforderungen, die bei jedem schulischen KI-Projekt eingehalten werden müssen.
* Die Studie „Pioniere des Wandels Wie Schüler:innen KI im Unterricht nutzen möchten“ der Vodafone-Stiftung bringt die Perspektive der Lernenden ein und bietet damit Anknüpfungspunkte für einen gemeinsamen Weg mit KI im Schulkontext.
* Der Beitrag „Audit-Style Framework for Evaluating Bias in Large Language Models“ stellt ein einfaches Drei-Phasen-Audit-Schema vor, das sich ohne Programmierkenntnisse auf Chatbots oder Textgeneratoren übertragen lässt.
* Das „AI Competency Framework for Teachers“ der UNESCO skizziert in fünf Kompetenzbereichen einen Stufenweg, mit dem Lehrkräfte künstliche Intelligenz menschenzentriert, inklusiv und ethisch reguliert in Unterricht und berufliche Weiterentwicklung integrieren können, um Lern- und Lehrergebnisse zu verbessern.
* Die Publikation „Vermeidung von Bias und Diskriminierung im Bereich Data Science“ erklärt, wie sich bereits bei der Datenvorbereitung Fairness-Ziele festlegen lassen, und ergänzt die schulische Perspektive um praktische Daten-Workflows.
* Der Artikel „Studie: Diskriminierung durch KI auch gesellschaftlicher Effekt“ liefert ein eingängiges Fallbeispiel, das den Zusammenhang zwischen Datenvielfalt und fairen Empfehlungen veranschaulicht.
* Die Expertise für den dritten Gleichstellungsbericht der Bundesregierung gibt Einblicke in „Geschlechterstereotype und soziale Medien“.
* Zum „Ethische[r]n Umgang mit Trainingsdaten: Für Fairness und Datenschutz in KI-Systemen“ vom LAMARR Institute for Machine Learning and Artificial Intelligence.
* „Künstliche neuronale Netze und Deep Learning einfach erklärt“ auf Kobold.ai.
* „Erklärbare KI: Das Geheimnis der Blackbox lüften“ vom Fraunhofer IAO.
* „Mangelnde Transparenz und Nachvollziehbarkeit von KI-Anwendungen“ auf bidt.digital.

**Stimmen aus der Praxis: Inwiefern verstärkt KI in sozialen Medien Vorurteile (Bias), und auf welche Weise zeigt sich das?**

Um Ihnen einen authentischen Einblick in die praktische Anwendung von KI zu geben, haben wir Expert*innen aus verschiedenen Bereichen zu Wort kommen lassen.

---

## Transferaufgaben

**Selbstreflexion – Bias im Sprachgebrauch erkennen**

Ziel dieser Aufgabe ist es, unbewusste Vorannahmen in häufig genutzten Formulierungen aufzudecken und durch wertschätzende, inklusive Sprache zu ersetzen.

* Sichten Sie Ihre letzten schriftlichen Rückmeldungen, Zeugnisbemerkungen oder Elternbriefe und wählen Sie fünf typische Sätze aus, die Sie in ähnlicher Form regelmäßig verwenden.
* Markieren Sie Wörter oder Wendungen, die Lernende, Eltern oder Kolleginnen und Kollegen in eine defizitorientierte Schublade stecken könnten (zum Beispiel schwach, wenig engagiert, schwieriger Hintergrund).
* Überlegen Sie, welche Annahmen hinter diesen Begriffen stehen und ob sie bestimmte Gruppen benachteiligen.
* Notieren Sie für jeden Satz in Stichpunkten, welche Wertung darin steckt.
* Formulieren Sie jede Bemerkung neu, sodass der Fokus auf beobachtbarem Verhalten, individuellen Fortschritten und konkreten Unterstützungsangeboten liegt.
* Nutzen Sie positive Verstärkung und vermeiden Sie stereotype Zuschreibungen.
* Vergleichen Sie Original und Überarbeitung in einer Tabelle und reflektieren Sie in zwei bis drei Sätzen, wie sich die Tonlage und der potenzielle Einfluss auf die Adressatinnen und Adressaten verändert haben.
* Durch diese Übung schärfen Sie Ihr Bewusstsein für sprachliche Verzerrungen und schaffen die Grundlage für faire, motivierende Kommunikation im Schul- und Verwaltungsalltag.

**Transferaufgabe – Prompt-Engineering**

Überarbeiten Sie mithilfe der Prompt-Engineering-Checkliste einen Prompt aus dem Schul- oder Verwaltungskontext.

Arbeiten Sie in drei Schritten:
1.  **Probleme markieren:** Unterstreichen Sie Formulierungen, die Vorannahmen oder sensible Merkmale enthalten.
2.  **Prompt optimieren:** Formulieren Sie so um, dass
    * sensible Merkmale neutralisiert werden,
    * positive Verstärkung statt Defizitfokus verwendet wird und
    * klare Qualitätskriterien (Tonfall, Umfang) vorgegeben sind.
3.  **Vorher – Nachher vergleichen:** Prüfen Sie mit mindestens zwei Testläufen, ob die Ausgaben fairer und konstruktiver werden.

*Beispielprompts für die Überarbeitung:*

* **Schulkontext (Zeugnisgenerator):** „Erstelle eine Zeugnisbemerkung für eine Schülerin, Gemeinschaftsschule, geringe Leistungen, mangelhafte Deutschkenntnisse.“
* **Schulkontext (Hausaufgabenvorschläge):** „Schlage einfache Matheaufgaben für schwache Schüler der Klasse 8 vor, damit sie wenigstens die Grundrechenarten üben können.“
* **Verwaltungskontext (Antwortmail des Sekretariats):** „Formuliere eine freundliche Antwort an die Mutter von Kevin, die nicht gut Deutsch spricht.“
* **Verwaltungskontext (Ticket-Priorisierung):** „Ordne diese Elternanfragen nach Wichtigkeit.“

---

## Zusammenfassung & Glossar

**Zusammenfassung**

Das Modul behandelt das Thema KI-Bias in sozialen Medien und zeigt, wie algorithmische Verzerrungen entstehen, wirken und erkannt werden können. Anhand von vier Videos werden zentrale Formen von Bias – insbesondere Input-Bias, verzerrte Datenqualität, versteckte Gewichtungen und Bias-Audits – erläutert.

Dabei wird deutlich, dass künstliche Intelligenz gesellschaftliche Vorurteile aus Trainingsdaten übernimmt und verstärkt. Durch Beispiele aus der Social Media-Nutzung lernen Teilnehmende, wie diese Mechanismen Meinungsbildung und Sichtbarkeit beeinflussen. Das Modul bietet praxisorientierte Ansätze, um Verzerrungen zu erkennen, kritisch zu hinterfragen und Jugendlichen Kompetenzen für einen reflektierten Umgang mit digitalen Medien zu vermitteln.