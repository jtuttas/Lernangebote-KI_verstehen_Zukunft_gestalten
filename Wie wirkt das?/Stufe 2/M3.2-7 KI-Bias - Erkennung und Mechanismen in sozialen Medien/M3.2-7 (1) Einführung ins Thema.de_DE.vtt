WEBVTT

1
00:00:06.720 --> 00:00:08.160
Herzlich willkommen zu diesem

2
00:00:08.160 --> 00:00:10.000
Schulungsvideo über KI-

3
00:00:10.000 --> 00:00:11.840
Bias in sozialen Medien.

4
00:00:12.800 --> 00:00:14.800
In diesem Video wird erläutert,

5
00:00:14.800 --> 00:00:17.440
wie künstliche Intelligenz das digitale

6
00:00:17.440 --> 00:00:19.960
Leben und insbesondere die Social Media

7
00:00:19.960 --> 00:00:22.480
Nutzung prägt, oft auf eine Weise,

8
00:00:22.480 --> 00:00:24.640
die kaum bewusst wahrgenommen wird.

9
00:00:28.250 --> 00:00:29.690
Zunächst die grundlegende Frage:

10
00:00:30.570 --> 00:00:32.810
Was ist eigentlich KI-Bias?

11
00:00:33.530 --> 00:00:35.770
Der Begriff bezeichnet systematische

12
00:00:35.770 --> 00:00:37.770
Verzerrungen in Algorithmen,

13
00:00:37.770 --> 00:00:40.090
die auf der Grundlage verzerrter oder

14
00:00:40.090 --> 00:00:42.970
unausgewogener Trainingsdaten entstehen.

15
00:00:42.970 --> 00:00:45.170
Diese Verzerrungen sind nicht zufällig,

16
00:00:45.170 --> 00:00:47.130
sondern folgen bestimmten Mustern.

17
00:00:47.850 --> 00:00:52.010
Ein wichtiger Punkt gleich zu beginn: KI-Bias

18
00:00:52.010 --> 00:00:54.170
unterscheidet sich von Zufallsfehlern.

19
00:00:55.200 --> 00:00:57.240
Während Zufallsfehler in alle Richtungen

20
00:00:57.240 --> 00:00:59.680
streuen, zeigen Bias Effekte

21
00:00:59.680 --> 00:01:01.280
immer in eine bestimmte Richtung.

22
00:01:01.840 --> 00:01:04.280
Sie bevorzugen oder benachteiligen

23
00:01:04.280 --> 00:01:05.720
systematisch bestimmte

24
00:01:05.720 --> 00:01:07.360
Gruppen oder Inhalte.

25
00:01:10.480 --> 00:01:12.640
Konkrete Beispiele aus dem digitalen

26
00:01:12.640 --> 00:01:14.833
Alltag verdeutlichen diese Mechanismen.

27
00:01:15.266 --> 00:01:16.206
Erstens:

28
00:01:16.400 --> 00:01:19.840
Einseitige Feeds auf Instagram, TikTok und YouTube.

29
00:01:20.410 --> 00:01:22.250
Die Algorithmen dieser Plattformen

30
00:01:22.250 --> 00:01:23.730
lernen aus dem Verhalten der

31
00:01:23.730 --> 00:01:26.170
Nutzenden und zeigen Inhalte, die

32
00:01:26.170 --> 00:01:28.010
vermeintlich am besten passen.

33
00:01:28.570 --> 00:01:30.370
Problematisch wird dies, wenn die

34
00:01:30.370 --> 00:01:32.170
Gewichtung nicht neutral ist,

35
00:01:32.170 --> 00:01:34.250
sondern durch die zugrundeliegenden

36
00:01:34.250 --> 00:01:36.610
Trainingsdaten, typische Verzerrungen

37
00:01:36.610 --> 00:01:38.410
durch KI-Bias sowie

38
00:01:38.410 --> 00:01:40.730
zielgruppenspezifische Anpassungen,

39
00:01:40.730 --> 00:01:43.930
im Sinne von Mikrotargeting hinzukommen.

40
00:01:43.930 --> 00:01:45.690
Die Vielfalt der Informationen

41
00:01:45.690 --> 00:01:47.640
nimmt ab und es entsteht ein

42
00:01:47.640 --> 00:01:49.680
verzerrtes Bild der Wirklichkeit.

43
00:01:50.366 --> 00:01:51.360
Zweitens:

44
00:01:51.360 --> 00:01:52.960
Beauty Filter, die

45
00:01:52.960 --> 00:01:54.480
automatisch aktiviert werden.

46
00:01:55.280 --> 00:01:56.880
Viele Nutzende berichten,

47
00:01:56.880 --> 00:01:58.600
dass ihre Gesichter ohne ihr

48
00:01:58.600 --> 00:02:00.240
Wissen verändert wurden.

49
00:02:00.240 --> 00:02:02.000
Ein besonders bekannter Fall

50
00:02:02.000 --> 00:02:05.760
ereignete sich 2021 bei TikTok.

51
00:02:05.760 --> 00:02:07.600
Der Algorithmus aktivierte

52
00:02:07.600 --> 00:02:09.919
automatisch Filter, die Gesichter

53
00:02:09.919 --> 00:02:11.960
verschönerten, ohne dass die

54
00:02:11.960 --> 00:02:13.600
Betroffenen dies gewollt hatten.

55
00:02:14.260 --> 00:02:15.780
Problematisch ist dies,

56
00:02:15.780 --> 00:02:17.860
weil dadurch ein unrealistisches

57
00:02:17.860 --> 00:02:20.260
Schönheitsideal vermittelt wird und das

58
00:02:20.260 --> 00:02:22.420
Selbstbild der Betroffenen negativ

59
00:02:22.420 --> 00:02:23.620
beeinflusst werden kann.

60
00:02:24.300 --> 00:02:24.980
Drittens:

61
00:02:24.980 --> 00:02:26.740
Empfehlungsalgorithmen, die

62
00:02:26.740 --> 00:02:28.900
Stereotype verstärken.

63
00:02:28.900 --> 00:02:31.100
Social Media Algorithmen greifen auf

64
00:02:31.100 --> 00:02:33.460
Trainingsdaten und Interaktionsmuster

65
00:02:33.460 --> 00:02:35.420
zurück, in denen traditionelle

66
00:02:35.420 --> 00:02:37.540
Rollenbilder stark vertreten sind.

67
00:02:38.180 --> 00:02:39.700
Dadurch werden diese Muster

68
00:02:39.700 --> 00:02:42.000
algorithmisch höher gewichtet

69
00:02:42.000 --> 00:02:43.920
und häufiger ausgespielt.

70
00:02:43.920 --> 00:02:46.160
Menschen, die viel Zeit in sozialen

71
00:02:46.160 --> 00:02:48.040
Netzwerken verbringen, neigen

72
00:02:48.040 --> 00:02:50.320
eher zu Stereotypenvorstellungen

73
00:02:50.320 --> 00:02:52.160
über Geschlechterrollen.

74
00:02:52.160 --> 00:02:54.920
Problematisch ist dies, weil der KI-

75
00:02:54.920 --> 00:02:57.760
Bias gesellschaftliche Vorurteile direkt

76
00:02:57.760 --> 00:03:00.200
in den Algorithmus einschreibt, sie

77
00:03:00.200 --> 00:03:02.880
verfestigt und die Vielfalt möglicher

78
00:03:02.880 --> 00:03:05.280
Lebensentwürfe unsichtbarer macht.

79
00:03:08.330 --> 00:03:10.010
An dieser Stelle ist es wichtig,

80
00:03:10.010 --> 00:03:11.690
zwei verwandte Begriffe zu

81
00:03:11.690 --> 00:03:12.533
unterscheiden.

82
00:03:12.810 --> 00:03:15.130
Bias und Echokammern.

83
00:03:15.130 --> 00:03:17.330
Bias bezeichnet systematische

84
00:03:17.330 --> 00:03:19.850
Verzerrungen durch unausgewogene

85
00:03:19.850 --> 00:03:22.330
Trainingsdaten in Algorithmen.

86
00:03:22.330 --> 00:03:24.690
Die Grundlage der Trainingsdaten sind

87
00:03:24.690 --> 00:03:26.250
Informationen, die von Menschen

88
00:03:26.250 --> 00:03:28.130
erstellt wurden und damit auch

89
00:03:28.130 --> 00:03:30.250
Vorurteile dieser Menschen abbilden.

90
00:03:30.810 --> 00:03:32.370
Die KI übernimmt und

91
00:03:32.370 --> 00:03:34.090
verstärkt diese Verzerrungen.

92
00:03:34.720 --> 00:03:36.480
Echokammern hingegen sind

93
00:03:36.480 --> 00:03:38.160
ein soziales Phänomen.

94
00:03:38.160 --> 00:03:39.800
Sie entstehen, wenn sich Menschen

95
00:03:39.800 --> 00:03:41.920
hauptsächlich mit Personen umgeben,

96
00:03:41.920 --> 00:03:44.080
die ähnlich denken oder ähnliche

97
00:03:44.080 --> 00:03:46.160
Informationsquellen nutzen.

98
00:03:46.160 --> 00:03:48.000
Besonders problematisch wird es,

99
00:03:48.000 --> 00:03:50.160
wenn beide Effekte zusammenwirken.

100
00:03:50.880 --> 00:03:52.920
KI-Algorithmen können

101
00:03:52.920 --> 00:03:55.280
Echokammer-Effekte verstärken.

102
00:03:55.280 --> 00:03:58.240
Ein Beispiel: Einseitige oder extrem

103
00:03:58.240 --> 00:04:00.720
polarisierende Inhalte werden nicht nur

104
00:04:00.720 --> 00:04:02.280
durch Algorithmen bevorzugt

105
00:04:02.280 --> 00:04:04.240
ausgespielt, sondern sondern auch über

106
00:04:04.240 --> 00:04:06.520
Echokammern weiter verbreitet.

107
00:04:06.520 --> 00:04:08.920
Das bedeutet, dass der Algorithmus

108
00:04:08.920 --> 00:04:11.640
bestimmte Inhalte immer wieder anzeigt,

109
00:04:11.640 --> 00:04:14.280
weil er sie als relevant einstuft,

110
00:04:14.280 --> 00:04:16.640
während die Mitglieder einer Echokammer

111
00:04:16.640 --> 00:04:18.240
diese Inhalte zusätzlich

112
00:04:18.240 --> 00:04:20.440
untereinander teilen und bestätigen.

113
00:04:21.000 --> 00:04:23.520
KI-Bots können diesen Mechanismus noch

114
00:04:23.520 --> 00:04:25.680
verstärken, indem sie Inhalte

115
00:04:25.680 --> 00:04:28.480
automatisch liken, kommentieren oder

116
00:04:28.480 --> 00:04:31.330
weiterverbreiten und damit zusätzliche

117
00:04:31.330 --> 00:04:33.890
künstliche Aufmerksamkeit erzeugen.

118
00:04:33.890 --> 00:04:35.050
So entsteht eine Art

119
00:04:35.050 --> 00:04:36.930
Rückkopplungsschleife.

120
00:04:36.930 --> 00:04:39.130
Je öfter ein falscher Inhalt erscheint

121
00:04:39.130 --> 00:04:41.490
und geteilt wird, desto glaubwürdiger

122
00:04:41.490 --> 00:04:43.650
wirkt er auf die Beteiligten.

123
00:04:43.650 --> 00:04:45.810
Dieser Verstärkungseffekt macht es sehr

124
00:04:45.810 --> 00:04:48.130
schwer, die Dynamik zu durchbrechen,

125
00:04:48.130 --> 00:04:49.650
weil gegen Informationen

126
00:04:49.650 --> 00:04:51.090
kaum noch wahrgenommen werden.

127
00:04:53.010 --> 00:04:55.170
Welche gesellschaftlichen Folgen haben

128
00:04:55.170 --> 00:04:56.500
diese Mechanismen?

129
00:04:56.866 --> 00:04:58.760
Erstens: Führen sie zur

130
00:04:58.760 --> 00:05:00.840
Diskriminierung marginalisierter

131
00:05:00.840 --> 00:05:02.440
Gruppen in sozialen Medien.

132
00:05:03.000 --> 00:05:05.280
KI Systeme können bestehende

133
00:05:05.280 --> 00:05:06.600
Ungleichheiten verstärken

134
00:05:06.600 --> 00:05:08.520
und neue schaffen.

135
00:05:08.520 --> 00:05:10.520
Beispielsweise werden Inhalte

136
00:05:10.520 --> 00:05:12.920
von Personen mit Migrationshintergrund

137
00:05:12.920 --> 00:05:14.560
oder aus benachteiligten

138
00:05:14.560 --> 00:05:16.760
Gruppen seltener ausgespielt.

139
00:05:16.760 --> 00:05:19.360
Ein Grund dafür ist, dass Trainingsdaten

140
00:05:19.360 --> 00:05:21.360
oft unausgewogen sind oder

141
00:05:21.360 --> 00:05:23.720
Algorithmen bestimmte Muster bevorzugen.

142
00:05:24.450 --> 00:05:25.890
Dadurch erscheinen Beiträge

143
00:05:25.890 --> 00:05:28.010
bestimmter Gruppen seltener und

144
00:05:28.010 --> 00:05:30.130
werden weniger sichtbar gemacht.

145
00:05:30.130 --> 00:05:32.050
Zweitens beeinflussen sie die

146
00:05:32.050 --> 00:05:33.890
Meinungsbildung von Menschen.

147
00:05:33.890 --> 00:05:35.810
Insbesondere Jugendliche sind hier

148
00:05:35.810 --> 00:05:37.810
gefährdet, weil sie Inhalte oft

149
00:05:37.810 --> 00:05:40.370
unreflektiert konsumieren und eine noch

150
00:05:40.370 --> 00:05:43.090
weniger gefestigte Persönlichkeit haben.

151
00:05:43.090 --> 00:05:45.490
Wenn Algorithmen immer ähnliche Inhalte

152
00:05:45.490 --> 00:05:47.650
zeigen, verengt sich der Horizont.

153
00:05:48.290 --> 00:05:49.770
Junge Menschen bekommen ein

154
00:05:49.770 --> 00:05:51.890
einseitiges Bild der Welt vermittelt.

155
00:05:52.860 --> 00:05:54.580
Drittens haben sie Auswirkungen

156
00:05:54.580 --> 00:05:56.900
auf die demokratische Teilhabe und

157
00:05:56.900 --> 00:05:58.540
den gesellschaftlichen Diskurs.

158
00:05:59.100 --> 00:06:00.820
Wenn verschiedene Gruppen völlig

159
00:06:00.820 --> 00:06:03.260
unterschiedliche Informationen erhalten,

160
00:06:03.260 --> 00:06:05.900
wird ein gemeinsamer Dialog schwieriger.

161
00:06:05.900 --> 00:06:07.820
Das bedeutet, dass Menschen mit

162
00:06:07.820 --> 00:06:09.860
konträren Informationsströmen

163
00:06:09.860 --> 00:06:11.580
nicht mehr auf einer gemeinsamen

164
00:06:11.580 --> 00:06:13.500
Wissensbasis diskutieren können.

165
00:06:14.220 --> 00:06:16.180
Diskussionen verlaufen zunehmend

166
00:06:16.180 --> 00:06:18.620
parallel, Missverständnisse nehmen zu

167
00:06:19.210 --> 00:06:20.490
und das Vertrauen in einen

168
00:06:20.490 --> 00:06:22.410
objektiven Austausch sinkt.

169
00:06:22.970 --> 00:06:24.490
Dadurch wird es schwerer,

170
00:06:24.490 --> 00:06:26.970
gesellschaftliche Kompromisse zu finden

171
00:06:26.970 --> 00:06:29.090
und ein gemeinsames Verständnis über

172
00:06:29.090 --> 00:06:30.890
Fakten und Werte zu entwickeln.

173
00:06:31.700 --> 00:06:32.490
Viertens,

174
00:06:32.490 --> 00:06:34.330
verstärken diese Mechanismen

175
00:06:34.330 --> 00:06:37.210
Stereotype und Vorurteile.

176
00:06:37.210 --> 00:06:39.770
Algorithmen greifen häufig auf bestehende

177
00:06:39.770 --> 00:06:42.050
Muster in den Daten zurück, wodurch

178
00:06:42.050 --> 00:06:44.370
gängige Rollenbilder oder Vorurteile

179
00:06:44.370 --> 00:06:45.850
unbewusst reproduziert werden.

180
00:06:46.700 --> 00:06:48.340
Was nach außen als neutrale

181
00:06:48.340 --> 00:06:50.180
Technik erscheint, transportiert

182
00:06:50.180 --> 00:06:51.420
damit gesellschaftliche

183
00:06:51.420 --> 00:06:53.500
Verzerrungen, verstärkt sie durch

184
00:06:53.500 --> 00:06:55.300
wiederholte Ausspielung und

185
00:06:55.300 --> 00:06:56.700
macht sie noch wirksamer.

186
00:06:57.260 --> 00:06:59.180
So können etwa stereotype

187
00:06:59.180 --> 00:07:01.260
Darstellungen von Geschlechterrollen,

188
00:07:01.260 --> 00:07:03.700
Schönheitsidealen oder kulturellen

189
00:07:03.700 --> 00:07:05.420
Zuschreibungen besonders sichtbar

190
00:07:05.420 --> 00:07:07.100
werden und langfristig das

191
00:07:07.100 --> 00:07:08.780
Denken der Nutzenden prägen.

192
00:07:12.070 --> 00:07:13.150
Diese Entwicklungen sind

193
00:07:13.150 --> 00:07:14.630
für Bildungsverantwortliche

194
00:07:14.630 --> 00:07:16.150
besonders relevant.

195
00:07:16.150 --> 00:07:18.110
Die Lebenswelt der Jugendlichen ist

196
00:07:18.110 --> 00:07:20.630
heute von Social Media geprägt.

197
00:07:20.630 --> 00:07:23.470
Erfahrungen, Weltbild und soziale

198
00:07:23.470 --> 00:07:25.390
Kontakte werden maßgeblich

199
00:07:25.390 --> 00:07:27.749
durch diese Plattformen geformt.

200
00:07:27.749 --> 00:07:29.710
Für Bildungseinrichtungen entsteht

201
00:07:29.710 --> 00:07:31.550
die Notwendigkeit, Jugendliche

202
00:07:31.550 --> 00:07:33.350
bei der Einordnung ihrer Social

203
00:07:33.350 --> 00:07:35.750
Media Erfahrungen zu unterstützen.

204
00:07:35.750 --> 00:07:38.000
Es geht nicht darum, soziale

205
00:07:38.000 --> 00:07:39.720
Medien und Smartphones pauschal

206
00:07:39.720 --> 00:07:41.440
zu verbieten, sondern

207
00:07:41.440 --> 00:07:44.200
die Förderung von Medienkompetenz.

208
00:07:44.200 --> 00:07:46.520
Konkret bedeutet das, Lehrkräfte

209
00:07:46.520 --> 00:07:48.600
und Mitarbeitende in der Verwaltung

210
00:07:48.600 --> 00:07:49.720
haben den Auftrag,

211
00:07:49.720 --> 00:07:51.560
kritisches Denken zu fördern.

212
00:07:52.120 --> 00:07:53.639
Ihre Rolle ist die von

213
00:07:53.639 --> 00:07:55.880
Begleitenden im digitalen Wandel.

214
00:07:59.240 --> 00:08:00.960
Zum Abschluss folgt ein Überblick

215
00:08:00.960 --> 00:08:03.080
über die Ursachen von KI-Bias.

216
00:08:03.933 --> 00:08:04.940
Erstens:

217
00:08:04.940 --> 00:08:07.580
Input Bias durch Trainingsdaten.

218
00:08:07.580 --> 00:08:09.420
Hierbei entstehen Verzerrungen,

219
00:08:09.420 --> 00:08:11.420
wenn Daten unausgewogen sind

220
00:08:11.420 --> 00:08:13.300
und bestimmte Gruppen oder Themen

221
00:08:13.300 --> 00:08:15.020
unterrepräsentiert bleiben.

222
00:08:15.266 --> 00:08:16.220
Zweitens:

223
00:08:16.220 --> 00:08:18.140
Meinungen in Hidden Layers.

224
00:08:18.140 --> 00:08:20.340
In den verborgenen Schichten neuronaler

225
00:08:20.340 --> 00:08:22.580
Netze bilden sich durch Gewichtungen

226
00:08:22.580 --> 00:08:24.700
gewissermaßen Meinungen, die

227
00:08:24.700 --> 00:08:26.860
zu systematischen Bevorzugungen

228
00:08:26.860 --> 00:08:28.940
oder Benachteiligungen führen können.

229
00:08:29.500 --> 00:08:30.470
Drittens:

230
00:08:30.470 --> 00:08:31.990
Mangelnde Transparenz und

231
00:08:31.990 --> 00:08:34.150
fehlende Bias Audits.

232
00:08:34.150 --> 00:08:37.270
Wenn KI-Systeme nicht überprüft werden,

233
00:08:37.270 --> 00:08:39.669
bleiben Verzerrungen unerkannt und

234
00:08:39.669 --> 00:08:42.230
können sich ungehindert fortsetzen.

235
00:08:42.230 --> 00:08:44.710
Zudem erschwert die mangelnde Transparenz

236
00:08:44.710 --> 00:08:46.190
die Nachvollziehbarkeit

237
00:08:46.190 --> 00:08:48.230
von Entscheidungen und verhindert,

238
00:08:48.230 --> 00:08:50.270
dass Lehrkräfte oder Lernende

239
00:08:50.270 --> 00:08:52.430
verstehen, auf welcher Grundlage

240
00:08:52.430 --> 00:08:54.790
die KI ihre Ergebnisse liefert.

241
00:08:58.020 --> 00:09:00.100
Zusammenfassend lässt sich sagen:

242
00:09:00.100 --> 00:09:01.940
KI-Bias bezeichnet

243
00:09:01.940 --> 00:09:03.460
systematische Verzerrungen

244
00:09:03.460 --> 00:09:04.940
in Algorithmen, die das

245
00:09:04.940 --> 00:09:07.380
digitale Leben beeinflussen.

246
00:09:07.380 --> 00:09:09.020
Diese Verzerrungen sind nicht

247
00:09:09.020 --> 00:09:10.899
neutral, sondern spiegeln

248
00:09:10.899 --> 00:09:12.980
gesellschaftliche Ungleichheiten wider

249
00:09:12.980 --> 00:09:14.420
und können sie verstärken.

250
00:09:15.060 --> 00:09:16.980
Für die Bildungsarbeit bedeutet dies,

251
00:09:17.700 --> 00:09:19.940
Jugendliche müssen unterstützt werden,

252
00:09:19.940 --> 00:09:21.940
diese Mechanismen zu verstehen

253
00:09:21.940 --> 00:09:23.620
und kritisch zu hinterfragen.
